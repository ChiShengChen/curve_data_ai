#!/usr/bin/env python3
"""
Curveæ™ºèƒ½é‡æ–°å¹³è¡¡æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse
import os
from datetime import datetime, timedelta
from typing import Tuple

from curve_rebalancer import CurvePoolPredictor, CurveDataCollector
from pathlib import Path

class CurveDataset(Dataset):
    """Curveæ•°æ®é›†ç±»"""
    
    def __init__(self, data: np.ndarray, targets: dict, seq_length: int = 24):
        self.data = data
        self.targets = targets
        self.seq_length = seq_length
        
    def __len__(self):
        return len(self.data) - self.seq_length
    
    def __getitem__(self, idx):
        # è¾“å…¥åºåˆ—
        x = self.data[idx:idx + self.seq_length]
        
        # ç›®æ ‡å€¼ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„çŠ¶æ€ï¼‰
        target_idx = idx + self.seq_length
        y = {
            'pool_balance': self.targets['pool_balance'][target_idx],
            'apy': self.targets['apy'][target_idx],
            'price_deviation': self.targets['price_deviation'][target_idx],
            'volume': self.targets['volume'][target_idx]
        }
        
        return torch.FloatTensor(x), y

def generate_synthetic_data(num_samples: int = 10000, seq_length: int = 24):
    """ç”Ÿæˆåˆæˆçš„Curveæ± æ•°æ®"""
    
    print(f"Generating {num_samples} synthetic data samples...")
    
    # æ—¶é—´åºåˆ—ç‰¹å¾
    dates = pd.date_range(start='2023-01-01', periods=num_samples, freq='H')
    
    # åŸºç¡€ç‰¹å¾
    data = {}
    
    # USDC, USDT, DAI ä½™é¢ (ç›¸äº’å…³è”)
    base_balance = 1000000
    usdc_trend = np.random.normal(0, 0.001, num_samples).cumsum()
    usdt_trend = np.random.normal(0, 0.001, num_samples).cumsum()
    dai_trend = -(usdc_trend + usdt_trend).astype(float) + np.random.normal(0, 0.0005, num_samples).cumsum()
    
    data['usdc_balance'] = base_balance * (1 + usdc_trend + 0.1 * np.sin(np.arange(num_samples) * 2 * np.pi / (24 * 7)))
    data['usdt_balance'] = base_balance * (1 + usdt_trend + 0.08 * np.cos(np.arange(num_samples) * 2 * np.pi / (24 * 7)))
    data['dai_balance'] = base_balance * (1 + dai_trend + 0.06 * np.sin(np.arange(num_samples) * 2 * np.pi / (24 * 30)))
    
    # Virtual price (é€šå¸¸ç¨³å®šå¢é•¿)
    data['virtual_price'] = 1.0 + np.random.normal(0, 0.0001, num_samples).cumsum() + 0.0001 * np.arange(num_samples)
    
    # äº¤æ˜“é‡ (æœ‰å‘¨æœŸæ€§å’Œéšæœºæ€§)
    volume_base = np.exp(np.random.normal(11, 1, num_samples))  # log-normalåˆ†å¸ƒ
    volume_weekly = 0.3 * np.sin(np.arange(num_samples) * 2 * np.pi / (24 * 7))  # å‘¨å‘¨æœŸ
    volume_daily = 0.2 * np.sin(np.arange(num_samples) * 2 * np.pi / 24)  # æ—¥å‘¨æœŸ
    data['volume_24h'] = volume_base * (1 + volume_weekly + volume_daily)
    
    df = pd.DataFrame(data, index=dates)
    
    # è®¡ç®—ç›®æ ‡å˜é‡
    targets = {}
    
    # æ± å­ä½™é¢æ¯”ä¾‹
    total_balance = df['usdc_balance'] + df['usdt_balance'] + df['dai_balance']
    targets['pool_balance'] = np.stack([
        df['usdc_balance'] / total_balance,
        df['usdt_balance'] / total_balance,
        df['dai_balance'] / total_balance
    ], axis=1)
    
    # APY (åŸºäºäº¤æ˜“é‡å’Œä½™é¢)
    targets['apy'] = (0.02 + 0.03 * (df['volume_24h'] / df['volume_24h'].mean()) / 
                     (total_balance / total_balance.mean())).values
    targets['apy'] = np.clip(targets['apy'], 0, 0.2)  # é™åˆ¶åœ¨0-20%
    
    # ä»·æ ¼åç¦» (åŸºäºä½™é¢ä¸å¹³è¡¡)
    ideal_balance = 1/3
    usdc_deviation = (df['usdc_balance'] / total_balance) - ideal_balance
    usdt_deviation = (df['usdt_balance'] / total_balance) - ideal_balance
    dai_deviation = (df['dai_balance'] / total_balance) - ideal_balance
    
    targets['price_deviation'] = np.stack([
        usdc_deviation * 0.01,  # è½¬æ¢ä¸ºä»·æ ¼åç¦»
        usdt_deviation * 0.01,
        dai_deviation * 0.01
    ], axis=1)
    
    # äº¤æ˜“é‡
    targets['volume'] = df['volume_24h'].values
    
    return df[['usdc_balance', 'usdt_balance', 'dai_balance', 'virtual_price', 'volume_24h']].values, targets

def load_real_csv_data(csv_data_dir: str) -> Tuple[np.ndarray, dict]:
    """ä»CSVæ–‡ä»¶åŠ è½½çœŸå®æ•°æ®"""
    
    data_dir = Path(csv_data_dir)
    historical_dir = data_dir / "historical"
    
    print(f"Loading real data from {historical_dir}...")
    
    if not historical_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {historical_dir}")
        print("è¯·å…ˆè¿è¡Œ python example_csv_usage.py è·å–æ•°æ®")
        return generate_synthetic_data(10000, 24)
    
    # æŸ¥æ‰¾3Poolçš„å†å²æ•°æ®æ–‡ä»¶
    csv_files = list(historical_dir.glob("3pool_historical_*.csv"))
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°3Poolå†å²æ•°æ®CSVæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ python example_csv_usage.py è·å–æ•°æ®")
        return generate_synthetic_data(10000, 24)
    
    # ä½¿ç”¨æœ€æ–°çš„CSVæ–‡ä»¶
    latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“Š ä½¿ç”¨æ•°æ®æ–‡ä»¶: {latest_csv.name}")
    
    try:
        df = pd.read_csv(latest_csv)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡çœŸå®æ•°æ®è®°å½•")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['usdc_balance', 'usdt_balance', 'dai_balance', 'virtual_price', 'volume_24h']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            print("ä½¿ç”¨åˆæˆæ•°æ®ä»£æ›¿...")
            return generate_synthetic_data(10000, 24)
        
        # æ•°æ®é¢„å¤„ç†
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        null_check = df[required_cols].isnull().sum().sum()
        if null_check > 0:
            print("âš ï¸  å‘ç°ç¼ºå¤±å€¼ï¼Œè¿›è¡Œå¡«å……...")
            df[required_cols] = df[required_cols].ffill().bfill()
        
        # æå–ç‰¹å¾
        X = df[required_cols].values
        
        # æ„é€ ç›®æ ‡å˜é‡ (ç±»ä¼¼åˆæˆæ•°æ®çš„æ–¹å¼)
        targets = {}
        
        # æ± å­ä½™é¢æ¯”ä¾‹
        total_balance = df['usdc_balance'] + df['usdt_balance'] + df['dai_balance']
        targets['pool_balance'] = np.stack([
            df['usdc_balance'] / total_balance,
            df['usdt_balance'] / total_balance, 
            df['dai_balance'] / total_balance
        ], axis=1)
        
        # APY (å¦‚æœæœ‰çš„è¯ï¼Œå¦åˆ™ä»volumeä¼°ç®—)
        if 'apy' in df.columns:
            targets['apy'] = df['apy'].values
        else:
            # åŸºäºäº¤æ˜“é‡ä¼°ç®—APY
            volume_normalized = df['volume_24h'] / df['volume_24h'].mean()
            tvl_normalized = total_balance / total_balance.mean()
            targets['apy'] = np.clip(0.02 + 0.03 * volume_normalized / tvl_normalized, 0, 0.2)
        
        # ä»·æ ¼åç¦» (åŸºäºä½™é¢æ¯”ä¾‹è®¡ç®—)
        ideal_balance = 1/3
        usdc_deviation = (df['usdc_balance'] / total_balance) - ideal_balance
        usdt_deviation = (df['usdt_balance'] / total_balance) - ideal_balance  
        dai_deviation = (df['dai_balance'] / total_balance) - ideal_balance
        
        targets['price_deviation'] = np.stack([
            usdc_deviation * 0.01,
            usdt_deviation * 0.01,
            dai_deviation * 0.01
        ], axis=1)
        
        # äº¤æ˜“é‡
        targets['volume'] = df['volume_24h'].values
        
        print("ğŸ“Š çœŸå®æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ•°æ®ç‚¹æ•°: {len(X)}")
        print(f"  - å¹³å‡Virtual Price: {df['virtual_price'].mean():.6f}")
        print(f"  - å¹³å‡æ—¥äº¤æ˜“é‡: ${df['volume_24h'].mean():,.0f}")
        apy_array = np.array(targets['apy'])
        print(f"  - APYèŒƒå›´: {apy_array.min():.2%} - {apy_array.max():.2%}")
        
        return X, targets
        
    except Exception as e:
        print(f"âŒ åŠ è½½CSVæ•°æ®å¤±è´¥: {e}")
        print("ä½¿ç”¨åˆæˆæ•°æ®ä»£æ›¿...")
        return generate_synthetic_data(10000, 24)

def train_model(args):
    """è®­ç»ƒæ¨¡å‹"""
    
    print("=== Curveæ™ºèƒ½é‡æ–°å¹³è¡¡æ¨¡å‹è®­ç»ƒ ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # ç”Ÿæˆæˆ–åŠ è½½æ•°æ®
    if args.use_real_data:
        print("ğŸŒ ä½¿ç”¨çœŸå®CSVæ•°æ®è¿›è¡Œè®­ç»ƒ...")
        X, targets = load_real_csv_data(args.csv_data_dir)
    else:
        print("ğŸ² ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒ...")
        X, targets = generate_synthetic_data(args.num_samples, args.seq_length)
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X)
    
    # åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * len(X_scaled))
    val_size = int(0.1 * len(X_scaled))
    
    X_train = X_scaled[:train_size]
    X_val = X_scaled[train_size:train_size + val_size]
    X_test = X_scaled[train_size + val_size:]
    
    # åˆ†å‰²ç›®æ ‡
    targets_train = {k: v[:train_size] for k, v in targets.items()}
    targets_val = {k: v[train_size:train_size + val_size] for k, v in targets.items()}
    targets_test = {k: v[train_size + val_size:] for k, v in targets.items()}
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = CurveDataset(X_train, targets_train, args.seq_length)
    val_dataset = CurveDataset(X_val, targets_val, args.seq_length)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CurvePoolPredictor(
        input_dim=X.shape[1],
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.7)
    
    # æŸå¤±å‡½æ•°
    mse_loss = nn.MSELoss()
    
    # è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    for epoch in range(args.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_train_loss = 0.0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]', leave=False)
        for batch_x, batch_y in train_pbar:
            batch_x = batch_x.to(device)
            
            # ç§»åŠ¨ç›®æ ‡åˆ°è®¾å¤‡
            batch_y_device = {}
            for key, value in batch_y.items():
                if key == 'pool_balance' or key == 'price_deviation':
                    batch_y_device[key] = value.float().to(device)
                else:
                    batch_y_device[key] = value.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            predictions = model(batch_x)
            
            # è®¡ç®—æŸå¤±
            loss = 0
            loss += mse_loss(predictions['pool_balance'], batch_y_device['pool_balance'])
            loss += mse_loss(predictions['apy'], batch_y_device['apy'])
            loss += mse_loss(predictions['price_deviation'], batch_y_device['price_deviation'])
            loss += mse_loss(predictions['volume'], batch_y_device['volume'])
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_train_loss += loss.item()
            train_pbar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0.0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Val]', leave=False)
            for batch_x, batch_y in val_pbar:
                batch_x = batch_x.to(device)
                
                batch_y_device = {}
                for key, value in batch_y.items():
                    if key == 'pool_balance' or key == 'price_deviation':
                        batch_y_device[key] = value.float().to(device)
                    else:
                        batch_y_device[key] = value.float().unsqueeze(1).to(device)
                
                predictions = model(batch_x)
                
                val_loss = 0
                val_loss += mse_loss(predictions['pool_balance'], batch_y_device['pool_balance'])
                val_loss += mse_loss(predictions['apy'], batch_y_device['apy'])
                val_loss += mse_loss(predictions['price_deviation'], batch_y_device['price_deviation'])
                val_loss += mse_loss(predictions['volume'], batch_y_device['volume'])
                
                epoch_val_loss += val_loss.item()
                val_pbar.set_postfix({'val_loss': f'{val_loss.item():.6f}'})
        
        avg_train_loss = epoch_train_loss / len(train_loader)
        avg_val_loss = epoch_val_loss / len(val_loader)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        
        print(f'Epoch {epoch+1:3d}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'scaler': scaler_X
            }, 'best_curve_model.pth')
            print(f"  â†’ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {best_val_loss:.6f})")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss', alpha=0.7)
    plt.plot(val_losses, label='Validation Loss', alpha=0.7)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Curve Model Training History')
    plt.grid(True, alpha=0.3)
    plt.savefig('training_curve.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: best_curve_model.pth")
    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: training_curve.png")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒCurveæ™ºèƒ½é‡æ–°å¹³è¡¡æ¨¡å‹')
    
    parser.add_argument('--num_samples', type=int, default=10000, 
                       help='ç”Ÿæˆçš„åˆæˆæ•°æ®æ ·æœ¬æ•°é‡')
    parser.add_argument('--seq_length', type=int, default=24, 
                       help='è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå°æ—¶ï¼‰')
    parser.add_argument('--batch_size', type=int, default=64, 
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=50, 
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001, 
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--hidden_dim', type=int, default=128, 
                       help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=2, 
                       help='LSTMå±‚æ•°')
    parser.add_argument('--use-real-data', action='store_true',
                       help='ä½¿ç”¨çœŸå®CSVæ•°æ®è€Œä¸æ˜¯åˆæˆæ•°æ®')
    parser.add_argument('--csv-data-dir', type=str, default='curve_data',
                       help='CSVæ•°æ®ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    train_model(args) 