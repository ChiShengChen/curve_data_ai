#!/usr/bin/env python3
"""
ğŸ”® Curve Virtual Priceé¢„æµ‹æ¨¡å‹
åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥24å°æ—¶çš„Virtual Priceå˜åŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class CurveVirtualPricePredictor:
    """Curveæ± å­Virtual Priceé¢„æµ‹å™¨"""
    
    def __init__(self, pool_name='3pool'):
        self.pool_name = pool_name
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        
    def load_data(self, file_path=None):
        """åŠ è½½å†å²æ•°æ®"""
        
        if file_path is None:
            file_path = f"free_historical_cache/{self.pool_name}_comprehensive_free_historical_365d.csv"
        
        try:
            self.data = pd.read_csv(file_path)
            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)} æ¡è®°å½•")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.data['timestamp'].min()} åˆ° {self.data['timestamp'].max()}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_features(self):
        """ç‰¹å¾å·¥ç¨‹ - åˆ›å»ºé¢„æµ‹ç‰¹å¾"""
        
        print("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        df = self.data.copy()
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # 1. æ»åç‰¹å¾ (Lag Features)
        for lag in [1, 6, 24, 168]:  # 1ä¸ªç‚¹ã€6ä¸ªç‚¹(1.5å°æ—¶)ã€24ä¸ªç‚¹(6å°æ—¶)ã€168ä¸ªç‚¹(42å°æ—¶/7å¤©)
            df[f'virtual_price_lag_{lag}'] = df['virtual_price'].shift(lag)
        
        # 2. ç§»åŠ¨å¹³å‡ç‰¹å¾ (Moving Average)
        for window in [24, 168, 672]:  # 6å°æ—¶ã€7å¤©ã€28å¤©
            df[f'virtual_price_ma_{window}'] = df['virtual_price'].rolling(window).mean()
        
        # 3. æ³¢åŠ¨ç‡ç‰¹å¾ (Volatility)
        for window in [24, 168]:
            df[f'virtual_price_std_{window}'] = df['virtual_price'].rolling(window).std()
            df[f'virtual_price_cv_{window}'] = df[f'virtual_price_std_{window}'] / df[f'virtual_price_ma_{window}']
        
        # 4. ä»·æ ¼å˜åŒ–ç‰¹å¾ (Price Change)
        df['virtual_price_change'] = df['virtual_price'].pct_change()
        df['virtual_price_change_abs'] = df['virtual_price_change'].abs()
        
        # 5. æµåŠ¨æ€§ç‰¹å¾ (Liquidity Features)
        df['total_supply_change'] = df['total_supply'].pct_change()
        df['total_supply_ma_24'] = df['total_supply'].rolling(24).mean()
        
        # 6. ä½™é¢ç‰¹å¾ (Balance Features)
        token_columns = [col for col in df.columns if col.endswith('_balance')]
        if len(token_columns) >= 2:
            df['balance_ratio'] = df[token_columns[0]] / df[token_columns[1]]
            df['balance_imbalance'] = df[token_columns].std(axis=1) / df[token_columns].mean(axis=1)
        
        # 7. æ—¶é—´ç‰¹å¾ (Time Features)
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        
        # 8. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (Technical Indicators)
        # RSI (ç›¸å¯¹å¼ºå¼±æŒ‡æ•°)
        df['price_change_positive'] = df['virtual_price_change'].apply(lambda x: x if x > 0 else 0)
        df['price_change_negative'] = df['virtual_price_change'].apply(lambda x: -x if x < 0 else 0)
        df['rsi_14'] = 100 - (100 / (1 + df['price_change_positive'].rolling(14).mean() / 
                                         df['price_change_negative'].rolling(14).mean()))
        
        # 9. ç›®æ ‡å˜é‡ (Target Variable)
        df['target_24h'] = df['virtual_price'].shift(-24)  # é¢„æµ‹24ä¸ªç‚¹åçš„ä»·æ ¼ (6å°æ—¶å)
        df['target_return_24h'] = (df['target_24h'] / df['virtual_price'] - 1) * 100  # æ”¶ç›Šç‡%
        
        # åˆ é™¤ç¼ºå¤±å€¼
        self.processed_data = df.dropna().reset_index(drop=True)
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
        print(f"ğŸ“Š å¤„ç†åæ•°æ®: {len(self.processed_data)} æ¡è®°å½•")
        print(f"ğŸ”§ ç‰¹å¾æ•°é‡: {len([col for col in df.columns if col not in ['timestamp', 'pool_address', 'pool_name', 'source', 'target_24h', 'target_return_24h']])} ä¸ª")
        
        return self.processed_data
    
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        exclude_cols = ['timestamp', 'pool_address', 'pool_name', 'source', 'target_24h', 'target_return_24h', 'virtual_price']
        self.feature_columns = [col for col in self.processed_data.columns if col not in exclude_cols]
        
        X = self.processed_data[self.feature_columns].fillna(0)
        y = self.processed_data['target_return_24h'].fillna(0)
        
        # æ—¶é—´åºåˆ—åˆ†å‰² (å‰80%è®­ç»ƒï¼Œå20%æµ‹è¯•)
        split_index = int(len(X) * 0.8)
        
        self.X_train = X.iloc[:split_index]
        self.X_test = X.iloc[split_index:]
        self.y_train = y.iloc[:split_index] 
        self.y_test = y.iloc[split_index:]
        
        # ç‰¹å¾ç¼©æ”¾
        self.X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(self.X_train),
            columns=self.X_train.columns,
            index=self.X_train.index
        )
        
        self.X_test_scaled = pd.DataFrame(
            self.scaler.transform(self.X_test),
            columns=self.X_test.columns,
            index=self.X_test.index
        )
        
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"ğŸ“Š æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        
    def train_model(self):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # ä½¿ç”¨Random Forestä½œä¸ºåŸºç¡€æ¨¡å‹
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(self.X_train_scaled, self.y_train)
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹
        train_pred = self.model.predict(self.X_train_scaled)
        test_pred = self.model.predict(self.X_test_scaled)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        train_mae = mean_absolute_error(self.y_train, train_pred)
        test_mae = mean_absolute_error(self.y_test, test_pred)
        
        train_rmse = np.sqrt(mean_squared_error(self.y_train, train_pred))
        test_rmse = np.sqrt(mean_squared_error(self.y_test, test_pred))
        
        # æ–¹å‘å‡†ç¡®ç‡
        train_direction_accuracy = np.mean(np.sign(train_pred) == np.sign(self.y_train)) * 100
        test_direction_accuracy = np.mean(np.sign(test_pred) == np.sign(self.y_test)) * 100
        
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"{'æŒ‡æ ‡':<15} {'è®­ç»ƒé›†':<12} {'æµ‹è¯•é›†':<12}")
        print("-" * 40)
        print(f"{'MAE':<15} {train_mae:<12.4f} {test_mae:<12.4f}")
        print(f"{'RMSE':<15} {train_rmse:<12.4f} {test_rmse:<12.4f}")
        print(f"{'æ–¹å‘å‡†ç¡®ç‡(%)':<15} {train_direction_accuracy:<12.1f} {test_direction_accuracy:<12.1f}")
        
        return {
            'train_mae': train_mae,
            'test_mae': test_mae,
            'train_rmse': train_rmse, 
            'test_rmse': test_rmse,
            'train_direction_acc': train_direction_accuracy,
            'test_direction_acc': test_direction_accuracy
        }
    
    def feature_importance(self, top_n=15):
        """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§"""
        
        if self.model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒ")
            return
            
        importances = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” Top {top_n} é‡è¦ç‰¹å¾:")
        print(importances.head(top_n).to_string(index=False))
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(10, 8))
        top_features = importances.head(top_n)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Feature Importance')
        plt.title(f'{self.pool_name} Virtual Priceé¢„æµ‹ - ç‰¹å¾é‡è¦æ€§')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return importances
    
    def predict_next_24h(self, current_data=None):
        """é¢„æµ‹æœªæ¥24å°æ—¶"""
        
        if self.model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒ")
            return None
            
        if current_data is None:
            # ä½¿ç”¨æœ€æ–°çš„æ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹
            current_data = self.X_test_scaled.iloc[-1:] 
        
        prediction = self.model.predict(current_data)
        
        print(f"ğŸ”® {self.pool_name} æœªæ¥6å°æ—¶Virtual Priceé¢„æµ‹:")
        print(f"   é¢„æœŸæ”¶ç›Šç‡: {prediction[0]:.4f}%")
        
        if prediction[0] > 0:
            print(f"   ğŸ“ˆ é¢„æµ‹ä¸Šæ¶¨ {prediction[0]:.4f}%")
        else:
            print(f"   ğŸ“‰ é¢„æµ‹ä¸‹è·Œ {abs(prediction[0]):.4f}%")
            
        return prediction[0]
    
    def plot_predictions(self, last_n_points=200):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        
        if self.model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒ")
            return
            
        # è·å–æµ‹è¯•é›†é¢„æµ‹
        test_pred = self.model.predict(self.X_test_scaled)
        
        # è·å–æœ€ånä¸ªç‚¹çš„æ•°æ®
        test_data = self.processed_data.iloc[len(self.X_train):].tail(last_n_points)
        actual_returns = self.y_test.tail(last_n_points).values
        pred_returns = test_pred[-last_n_points:]
        
        plt.figure(figsize=(15, 8))
        
        # ç»˜åˆ¶å®é™…vsé¢„æµ‹æ”¶ç›Šç‡
        plt.subplot(2, 1, 1)
        plt.plot(actual_returns, label='å®é™…æ”¶ç›Šç‡', alpha=0.7)
        plt.plot(pred_returns, label='é¢„æµ‹æ”¶ç›Šç‡', alpha=0.7)
        plt.title(f'{self.pool_name} Virtual Priceæ”¶ç›Šç‡é¢„æµ‹ (æœ€è¿‘{last_n_points}ä¸ªæ—¶é—´ç‚¹)')
        plt.ylabel('æ”¶ç›Šç‡ (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶é¢„æµ‹è¯¯å·®
        plt.subplot(2, 1, 2)
        errors = actual_returns - pred_returns
        plt.plot(errors, color='red', alpha=0.7)
        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        plt.title('é¢„æµ‹è¯¯å·®')
        plt.ylabel('è¯¯å·® (%)')
        plt.xlabel('æ—¶é—´ç‚¹')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_prediction_results.png', dpi=300, bbox_inches='tight')
        plt.show()

def demo_virtual_price_prediction():
    """æ¼”ç¤ºVirtual Priceé¢„æµ‹"""
    
    print("ğŸ”® Curve Virtual Priceé¢„æµ‹æ¨¡å‹æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = CurveVirtualPricePredictor(pool_name='3pool')
    
    # åŠ è½½æ•°æ®
    if not predictor.load_data():
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®ä¿æœ‰æ•°æ®æ–‡ä»¶")
        return
    
    # ç‰¹å¾å·¥ç¨‹
    predictor.create_features()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    predictor.prepare_training_data()
    
    # è®­ç»ƒæ¨¡å‹
    predictor.train_model()
    
    # è¯„ä¼°æ¨¡å‹
    metrics = predictor.evaluate_model()
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    importance = predictor.feature_importance()
    
    # é¢„æµ‹æœªæ¥24å°æ—¶
    prediction = predictor.predict_next_24h()
    
    # å¯è§†åŒ–ç»“æœ
    predictor.plot_predictions()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Virtual Priceé¢„æµ‹æ¨¡å‹æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“Š æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {metrics['test_direction_acc']:.1f}%")
    print(f"ğŸ“ˆ é¢„æµ‹æ”¶ç›Šç‡: {prediction:.4f}%")
    print("=" * 50)

if __name__ == "__main__":
    demo_virtual_price_prediction() 