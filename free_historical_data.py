#!/usr/bin/env python3
"""
å…è´¹å†å²æ•°æ®è·å–ç­–ç•¥
ä¸“é—¨é’ˆå¯¹ä¸æƒ³èŠ±é’±ä½†éœ€è¦å†å²æ•°æ®çš„åœºæ™¯
"""

import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
from pathlib import Path

# ========================================
# ğŸ”§ é…ç½®å‚æ•° - åœ¨è¿™é‡Œä¿®æ”¹å¤©æ•°è®¾ç½®
# ========================================
DEFAULT_HISTORICAL_DAYS = 365  # é»˜è®¤è·å–1å¹´å†å²æ•°æ®
MAX_THEGRAPH_DAYS = 365       # The Graph APIæœ€å¤§æ”¯æŒå¤©æ•° (å·²åºŸå¼ƒ)
DEFAULT_SELF_BUILT_DAYS = 365 # è‡ªå»ºæ•°æ®åº“é»˜è®¤å¤©æ•°

# å¿«é€Ÿé…ç½®é€‰é¡¹
QUICK_TEST_DAYS = 7           # å¿«é€Ÿæµ‹è¯•ç”¨(7å¤©)
MEDIUM_RANGE_DAYS = 90        # ä¸­æœŸåˆ†æç”¨(3ä¸ªæœˆ) 
FULL_YEAR_DAYS = 365          # å®Œæ•´å¹´åº¦æ•°æ®

# å½“å‰ä½¿ç”¨çš„é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥æ”¹å˜æ‰€æœ‰æ–¹æ³•çš„é»˜è®¤å€¼
CURRENT_DAYS_SETTING =  FULL_YEAR_DAYS  # ğŸ”¥ æš‚æ—¶æ”¹ä¸º7å¤©é¿å…é•¿æ—¶é—´ç­‰å¾…

# ========================================
# ğŸš¨ API çŠ¶æ€é…ç½® - æ§åˆ¶å“ªäº›æ•°æ®æºå¯ç”¨
# ========================================
ENABLE_THEGRAPH_API = False     # âŒ The Graph APIå·²è¢«ç§»é™¤ï¼Œç¦ç”¨
ENABLE_CURVE_API = True         # âœ… Curve API (ä¸»è¦æ•°æ®æº)
ENABLE_DEFILLAMA = True         # âœ… DefiLlama APYæ•°æ®  
ENABLE_SELF_BUILT = True        # âœ… è‡ªå»ºæ•°æ®åº“ (ä½œä¸ºæœ€åå¤‡ä»½)
ENABLE_SSL_VERIFICATION = False # ğŸ”§ ç¦ç”¨SSLéªŒè¯æ¥é¿å…è¯ä¹¦é”™è¯¯

# æ€§èƒ½ä¼˜åŒ–é…ç½®
MAX_COLLECTION_ATTEMPTS = 50   # ğŸ”¥ é™åˆ¶æœ€å¤§æ”¶é›†æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
COLLECTION_BATCH_SIZE = 10     # æ¯æ‰¹æ¬¡æ”¶é›†çš„æ•°æ®ç‚¹æ•°é‡
REQUEST_TIMEOUT = 5            # APIè¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
REQUEST_RETRY_DELAY = 2        # è¯·æ±‚å¤±è´¥åé‡è¯•å»¶è¿Ÿ (ç§’)

# ========================================
# ğŸ¯ æ‰€æœ‰ä¸»è¦Curveæ± å­é…ç½® - æ‰©å±•ç‰ˆ
# ========================================

# å®Œæ•´çš„ä¸»è¦Curveæ± å­é…ç½®
AVAILABLE_POOLS = {
    # === ğŸ† ä¸»è¦ç¨³å®šå¸æ±  (Base Pools) ===
    '3pool': {
        'address': '0xbEbc44782C7dB0a1A60Cb6fe97d0b483032FF1C7',
        'name': 'DAI/USDC/USDT',
        'tokens': ['DAI', 'USDC', 'USDT'],
        'type': 'stable',
        'priority': 1  # æœ€é«˜ä¼˜å…ˆçº§
    },
    'frax': {
        'address': '0xd632f22692FaC7611d2AA1C0D552930D43CAEd3B', 
        'name': 'FRAX/3CRV',
        'tokens': ['FRAX', '3CRV'],
        'type': 'metapool',
        'priority': 2
    },
    'lusd': {
        'address': '0xEd279fDD11cA84bEef15AF5D39BB4d4bEE23F0cA',
        'name': 'LUSD/3CRV', 
        'tokens': ['LUSD', '3CRV'],
        'type': 'metapool',
        'priority': 2
    },
    'mim': {
        'address': '0x5a6A4D54456819380173272A5E8E9B9904BdF41B',
        'name': 'MIM/3CRV',
        'tokens': ['MIM', '3CRV'], 
        'type': 'metapool',
        'priority': 3
    },
    
    # === ğŸ”¥ ETH/stETH æ±  ===
    'steth': {
        'address': '0xDC24316b9AE028F1497c275EB9192a3Ea0f67022',
        'name': 'ETH/stETH',
        'tokens': ['ETH', 'stETH'],
        'type': 'eth_pool',
        'priority': 2
    },
    'seth': {
        'address': '0xc5424B857f758E906013F3555Dad202e4bdB4567',
        'name': 'ETH/sETH',
        'tokens': ['ETH', 'sETH'],
        'type': 'eth_pool', 
        'priority': 3
    },
    'reth': {
        'address': '0xF9440930043eb3997fc70e1339dBb11F341de7A8',
        'name': 'ETH/rETH',
        'tokens': ['ETH', 'rETH'],
        'type': 'eth_pool',
        'priority': 3
    },
    'ankrETH': {
        'address': '0xA96A65c051bF88B4095Ee1f2451C2A9d43F53Ae2',
        'name': 'ETH/ankrETH', 
        'tokens': ['ETH', 'ankrETH'],
        'type': 'eth_pool',
        'priority': 4
    },
    
    # === â‚¿ BTC æ±  ===
    'renbtc': {
        'address': '0x93054188d876f558f4a66B2EF1d97d16eDf0895B',
        'name': 'renBTC/WBTC',
        'tokens': ['renBTC', 'WBTC'],
        'type': 'btc_pool',
        'priority': 3
    },
    'sbtc': {
        'address': '0x7fC77b5c7614E1533320Ea6DDc2Eb61fa00A9714',
        'name': 'renBTC/WBTC/sBTC',
        'tokens': ['renBTC', 'WBTC', 'sBTC'],
        'type': 'btc_pool',
        'priority': 3
    },
    'hbtc': {
        'address': '0x4CA9b3063Ec5866A4B82E437059D2C43d1be596F',
        'name': 'hBTC/WBTC',
        'tokens': ['hBTC', 'WBTC'],
        'type': 'btc_pool',
        'priority': 4
    },
    'bbtc': {
        'address': '0x071c661B4DeefB59E2a3DdB20Db036821eeE8F4b',
        'name': 'bBTC/sbtcCRV',
        'tokens': ['bBTC', 'sbtcCRV'],
        'type': 'btc_metapool',
        'priority': 4
    },
    'obtc': {
        'address': '0xd81dA8D904b52208541Bade1bD6595D8a251F8dd',
        'name': 'oBTC/sbtcCRV',
        'tokens': ['oBTC', 'sbtcCRV'],
        'type': 'btc_metapool', 
        'priority': 4
    },
    'pbtc': {
        'address': '0x7F55DDe206dbAD629C080068923b36fe9D6bDBeF',
        'name': 'pBTC/sbtcCRV',
        'tokens': ['pBTC', 'sbtcCRV'],
        'type': 'btc_metapool',
        'priority': 4
    },
    'tbtc': {
        'address': '0xC25099792E9349C7DD09759744ea681C7de2cb66',
        'name': 'tBTC/sbtcCRV', 
        'tokens': ['tBTC', 'sbtcCRV'],
        'type': 'btc_metapool',
        'priority': 4
    },
    
    # === ğŸš€ Crypto æ±  ===
    'tricrypto': {
        'address': '0x80466c64868E1ab14a1Ddf27A676C3fcBE638Fe5',
        'name': 'USDT/WBTC/WETH',
        'tokens': ['USDT', 'WBTC', 'WETH'],
        'type': 'crypto',
        'priority': 2
    },
    'tricrypto2': {
        'address': '0xD51a44d3FaE010294C616388b506AcDA1bfAAE46', 
        'name': 'USDT/WBTC/WETH v2',
        'tokens': ['USDT', 'WBTC', 'WETH'],
        'type': 'crypto',
        'priority': 2
    },
    
    # === ğŸ¦ Lending æ±  ===
    'aave': {
        'address': '0xDeBF20617708857ebe4F679508E7b7863a8A8EeE',
        'name': 'aDAI/aUSDC/aUSDT',
        'tokens': ['aDAI', 'aUSDC', 'aUSDT'],
        'type': 'lending',
        'priority': 3
    },
    'compound': {
        'address': '0xA2B47E3D5c44877cca798226B7B8118F9BFb7A56',
        'name': 'cDAI/cUSDC',
        'tokens': ['cDAI', 'cUSDC'],
        'type': 'lending',
        'priority': 4
    },
    'ironbank': {
        'address': '0x2dded6Da1BF5DBdF597C45fcFaa3194e53EcfeAF',
        'name': 'cyDAI/cyUSDC/cyUSDT',
        'tokens': ['cyDAI', 'cyUSDC', 'cyUSDT'],
        'type': 'lending',
        'priority': 4
    },
    'saave': {
        'address': '0xEB16Ae0052ed37f479f7fe63849198Df1765a733',
        'name': 'sDAI/sUSDC/sUSDT',
        'tokens': ['sDAI', 'sUSDC', 'sUSDT'],
        'type': 'lending',
        'priority': 4
    },
    
    # === ğŸŒ å›½é™…åŒ–ç¨³å®šå¸ ===
    'eurs': {
        'address': '0x0Ce6a5fF5217e38315f87032CF90686C96627CAA',
        'name': 'EURS/sEUR',
        'tokens': ['EURS', 'sEUR'],
        'type': 'international',
        'priority': 4
    },
    
    # === ğŸ“ˆ æ›´å¤šMetaæ±  ===
    'gusd': {
        'address': '0x4f062658EaAF2C1ccf8C8e36D6824CDf41167956',
        'name': 'GUSD/3CRV',
        'tokens': ['GUSD', '3CRV'],
        'type': 'metapool',
        'priority': 4
    },
    'husd': {
        'address': '0x3eF6A01A0f81D6046290f3e2A8c5b843e738E604',
        'name': 'HUSD/3CRV',
        'tokens': ['HUSD', '3CRV'],
        'type': 'metapool',
        'priority': 4
    },
    'musd': {
        'address': '0x8474DdbE98F5aA3179B3B3F5942D724aFcdec9f6',
        'name': 'MUSD/3CRV',
        'tokens': ['MUSD', '3CRV'],
        'type': 'metapool',
        'priority': 4
    },
    'dusd': {
        'address': '0x8038C01A0390a8c547446a0b2c18fc9aEFEcc10c',
        'name': 'DUSD/3CRV',
        'tokens': ['DUSD', '3CRV'],
        'type': 'metapool',
        'priority': 5
    },
    'usdk': {
        'address': '0x3E01dD8a5E1fb3481F0F589056b428Fc308AF0Fb',
        'name': 'USDK/3CRV',
        'tokens': ['USDK', '3CRV'],
        'type': 'metapool',
        'priority': 5
    },
    'usdn': {
        'address': '0x0f9cb53Ebe405d49A0bbdBD291A65Ff571bC83e1',
        'name': 'USDN/3CRV',
        'tokens': ['USDN', '3CRV'],
        'type': 'metapool',
        'priority': 5
    },
    'usdp': {
        'address': '0x42d7025938bEc20B69cBae5A77421082407f053A',
        'name': 'USDP/3CRV',
        'tokens': ['USDP', '3CRV'],
        'type': 'metapool',
        'priority': 4
    },
    'ust': {
        'address': '0x890f4e345B1dAED0367A877a1612f86A1f86985f',
        'name': 'UST/3CRV',
        'tokens': ['UST', '3CRV'],
        'type': 'metapool',
        'priority': 5  # è¾ƒä½ä¼˜å…ˆçº§å› ä¸ºUSTå·²deprecated
    },
    'rsv': {
        'address': '0xC18cC39da8b11dA8c3541C598eE022258F9744da',
        'name': 'RSV/3CRV',
        'tokens': ['RSV', '3CRV'],
        'type': 'metapool',
        'priority': 5
    },
    'linkusd': {
        'address': '0xE7a24EF0C5e95Ffb0f6684b813A78F2a3AD7D171',
        'name': 'LINKUSD/3CRV',
        'tokens': ['LINKUSD', '3CRV'],
        'type': 'metapool',
        'priority': 5
    },
    
    # === ğŸ”— å…¶ä»–é‡è¦æ± å­ ===
    'link': {
        'address': '0xF178C0b5Bb7e7aBF4e12A4838C7b7c5bA2C623c0',
        'name': 'LINK/sLINK',
        'tokens': ['LINK', 'sLINK'],
        'type': 'synthetic',
        'priority': 4
    },
    'susd': {
        'address': '0xA5407eAE9Ba41422680e2e00537571bcC53efBfD',
        'name': 'DAI/USDC/USDT/sUSD',
        'tokens': ['DAI', 'USDC', 'USDT', 'sUSD'],
        'type': 'stable_4pool',
        'priority': 4
    },
    'y': {
        'address': '0x45F783CCE6B7FF23B2ab2D70e416cdb7D6055f51',
        'name': 'yDAI/yUSDC/yUSDT/yTUSD',
        'tokens': ['yDAI', 'yUSDC', 'yUSDT', 'yTUSD'],
        'type': 'yield',
        'priority': 5
    },
    'busd': {
        'address': '0x79a8C46DeA5aDa233ABaFFD40F3A0A2B1e5A4F27',
        'name': 'yDAI/yUSDC/yUSDT/yBUSD',
        'tokens': ['yDAI', 'yUSDC', 'yUSDT', 'yBUSD'],
        'type': 'yield',
        'priority': 5
    },
    'pax': {
        'address': '0x06364f10B501e868329afBc005b3492902d6C763',
        'name': 'ycDAI/ycUSDC/ycUSDT/PAX',
        'tokens': ['ycDAI', 'ycUSDC', 'ycUSDT', 'PAX'],
        'type': 'yield',
        'priority': 5
    }
}

# æ ¹æ®ä¼˜å…ˆçº§å’Œç±»å‹ç­›é€‰æ± å­çš„å‡½æ•°
def get_pools_by_priority(min_priority=1, max_priority=5, pool_types=None):
    """
    æ ¹æ®ä¼˜å…ˆçº§å’Œç±»å‹ç­›é€‰æ± å­
    
    Args:
        min_priority: æœ€å°ä¼˜å…ˆçº§ (1=æœ€é«˜ä¼˜å…ˆçº§)  
        max_priority: æœ€å¤§ä¼˜å…ˆçº§ (5=æœ€ä½ä¼˜å…ˆçº§)
        pool_types: æ± å­ç±»å‹åˆ—è¡¨ï¼Œå¦‚ ['stable', 'metapool'] 
    
    Returns:
        ç­›é€‰åçš„æ± å­å­—å…¸
    """
    filtered_pools = {}
    
    for pool_name, pool_info in AVAILABLE_POOLS.items():
        # ä¼˜å…ˆçº§ç­›é€‰
        if not (min_priority <= pool_info['priority'] <= max_priority):
            continue
            
        # ç±»å‹ç­›é€‰
        if pool_types and pool_info['type'] not in pool_types:
            continue
            
        filtered_pools[pool_name] = pool_info
    
    return filtered_pools

def get_high_priority_pools():
    """è·å–é«˜ä¼˜å…ˆçº§æ± å­ (priority 1-2)"""
    return get_pools_by_priority(min_priority=1, max_priority=2)

def get_stable_pools():
    """è·å–æ‰€æœ‰ç¨³å®šå¸ç›¸å…³æ± å­"""
    return get_pools_by_priority(pool_types=['stable', 'metapool', 'stable_4pool'])

def get_all_main_pools():
    """è·å–æ‰€æœ‰ä¸»è¦æ± å­ (priority 1-3)"""  
    return get_pools_by_priority(min_priority=1, max_priority=3)

# æ›´æ–°åŸæœ‰é…ç½®ä»¥ä¿æŒå…¼å®¹æ€§
TARGET_POOL = '3pool'  # é»˜è®¤æ± å­
TARGET_POOL_ADDRESS = AVAILABLE_POOLS[TARGET_POOL]['address']
TARGET_POOL_NAME = AVAILABLE_POOLS[TARGET_POOL]['name']

# æ‰¹é‡æ¨¡å¼é…ç½®
ENABLE_BATCH_MODE = False     # æ˜¯å¦å¯ç”¨æ‰¹é‡æ¨¡å¼ (æ”¶é›†æ‰€æœ‰æ± å­)
BATCH_POOLS = ['3pool', 'frax', 'lusd']  # æ‰¹é‡æ¨¡å¼æ—¶è¦æ”¶é›†çš„æ± å­åˆ—è¡¨

# æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
def show_current_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    print("=" * 60)
    print("ğŸ“‹ å½“å‰çˆ¬è™«é…ç½®")
    print("=" * 60)
    print(f"ğŸ¯ ç›®æ ‡æ± å­: {TARGET_POOL}")
    print(f"ğŸ“› æ± å­åç§°: {TARGET_POOL_NAME}")  
    print(f"ğŸ“ æ± å­åœ°å€: {TARGET_POOL_ADDRESS}")
    print(f"ğŸ“Š æ•°æ®å¤©æ•°: {CURRENT_DAYS_SETTING} å¤©")
    print(f"ğŸ”„ æ‰¹é‡æ¨¡å¼: {'å¯ç”¨' if ENABLE_BATCH_MODE else 'ç¦ç”¨'}")
    if ENABLE_BATCH_MODE:
        print(f"ğŸ“¦ æ‰¹é‡æ± å­: {', '.join(BATCH_POOLS)}")
    print("=" * 60)
    print("ğŸ’¡ è¦åˆ‡æ¢æ± å­ï¼Œè¯·ä¿®æ”¹ TARGET_POOL å˜é‡!")
    print("   å¯é€‰å€¼: " + " | ".join(AVAILABLE_POOLS.keys()))
    print("=" * 60)

# é…ç½®ä¿¡æ¯å°†åœ¨ä¸»ç¨‹åºè¿è¡Œæ—¶æ˜¾ç¤º

class FreeHistoricalDataManager:
    """å…è´¹å†å²æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "free_historical_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # å…è´¹æ•°æ®æº
        self.sources = {
            'thegraph': {
                'url': 'https://api.thegraph.com/subgraphs/name/messari/curve-finance-ethereum',
                'daily_limit': 1000,
                'cost': 'FREE'
            },
            'curve_api': {
                'url': 'https://api.curve.fi/api',
                'daily_limit': 'unlimited',
                'cost': 'FREE'
            },
            'defillama': {
                'url': 'https://yields.llama.fi',
                'daily_limit': 'unlimited', 
                'cost': 'FREE'
            }
        }
        
        print(f"ğŸ“ å…è´¹å†å²æ•°æ®ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
    
    def get_thegraph_historical_data(self, pool_address: str, days: int = CURRENT_DAYS_SETTING) -> pd.DataFrame:
        """
        æ–¹æ³•1: ä½¿ç”¨The Graphè·å–å†å²æ•°æ® (å·²åºŸå¼ƒ)
        âŒ æ³¨æ„: The Graph APIç«¯ç‚¹å·²è¢«ç§»é™¤
        """
        
        if not ENABLE_THEGRAPH_API:
            print(f"âš ï¸  [The Graph] APIå·²è¢«ç¦ç”¨ - ç«¯ç‚¹å·²åºŸå¼ƒ")
            return pd.DataFrame()
        
        print(f"ğŸ“Š [The Graph] å°è¯•è·å– {days} å¤©å†å²æ•°æ®...")
        
        # GraphQLæŸ¥è¯¢ - åˆ†æ‰¹è·å–é¿å…è¶…æ—¶
        query = """
        {
          pool(id: "%s") {
            name
            coins {
              symbol
              decimals
            }
            dailyPoolSnapshots(
              first: %d
              orderBy: timestamp
              orderDirection: desc
            ) {
              timestamp
              totalValueLockedUSD
              dailyVolumeUSD
              rates
              balances
              virtualPrice
            }
          }
        }
        """ % (pool_address.lower(), days)
        
        try:
            # é…ç½®SSLéªŒè¯
            verify_ssl = ENABLE_SSL_VERIFICATION
            
            response = requests.post(
                self.sources['thegraph']['url'],
                json={'query': query},
                timeout=REQUEST_TIMEOUT,
                headers={'Content-Type': 'application/json'},
                verify=verify_ssl
            )
            
            if response.status_code != 200:
                print(f"âŒ The Graph APIé”™è¯¯: {response.status_code}")
                return pd.DataFrame()
            
            data = response.json()
            
            if 'errors' in data:
                print(f"âŒ GraphQLé”™è¯¯ (APIå·²åºŸå¼ƒ): {data['errors'][0]['message'][:100]}...")
                return pd.DataFrame()
            
            pool_data = data['data']['pool']
            if not pool_data:
                print(f"âŒ æœªæ‰¾åˆ°æ± å­æ•°æ®: {pool_address}")
                return pd.DataFrame()
            
            # è§£ææ•°æ®
            records = []
            snapshots = pool_data['dailyPoolSnapshots']
            coins = pool_data['coins']
            
            for snapshot in snapshots:
                record = {
                    'timestamp': pd.to_datetime(int(snapshot['timestamp']), unit='s'),
                    'tvl': float(snapshot.get('totalValueLockedUSD', 0)),
                    'volume_24h': float(snapshot.get('dailyVolumeUSD', 0)),
                    'virtual_price': float(snapshot.get('virtualPrice', 1e18)) / 1e18
                }
                
                # è§£æä½™é¢æ•°æ®
                balances = snapshot.get('balances', [])
                rates = snapshot.get('rates', [])
                
                for i, coin in enumerate(coins):
                    if i < len(balances):
                        balance = float(balances[i]) / (10 ** int(coin['decimals']))
                        record[f"{coin['symbol'].lower()}_balance"] = balance
                    
                    if i < len(rates):
                        record[f"{coin['symbol'].lower()}_rate"] = float(rates[i]) / 1e18
                
                records.append(record)
            
            df = pd.DataFrame(records)
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            print(f"âœ… [The Graph] è·å–åˆ° {len(df)} æ¡å…è´¹å†å²è®°å½•")
            return df
            
        except Exception as e:
            print(f"âŒ The GraphæŸ¥è¯¢å¤±è´¥ (APIå·²åºŸå¼ƒ): {str(e)[:100]}...")
            return pd.DataFrame()
    
    def get_defillama_apy_history(self, pool_address: str) -> pd.DataFrame:
        """
        æ–¹æ³•2: ä»DefiLlamaè·å–APYå†å² (å®Œå…¨å…è´¹)
        """
        
        if not ENABLE_DEFILLAMA:
            print(f"âš ï¸  [DefiLlama] APIå·²è¢«ç¦ç”¨")
            return pd.DataFrame()
            
        print(f"ğŸ“ˆ [DefiLlama] è·å–APYå†å²æ•°æ®...")
        
        try:
            # è·å–æ± å­APYå†å²
            url = f"https://yields.llama.fi/chart/{pool_address.lower()}"
            
            # é…ç½®SSLéªŒè¯å’Œè¶…æ—¶
            verify_ssl = ENABLE_SSL_VERIFICATION
            
            response = requests.get(
                url, 
                timeout=REQUEST_TIMEOUT,
                verify=verify_ssl
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and data['data']:
                    records = []
                    for item in data['data']:
                        records.append({
                            'timestamp': pd.to_datetime(item['timestamp']),
                            'apy': item['apy'] / 100 if item['apy'] else 0,
                            'tvl': item.get('tvlUsd', 0)
                        })
                    
                    df = pd.DataFrame(records)
                    print(f"âœ… [DefiLlama] è·å–åˆ° {len(df)} æ¡APYå†å²è®°å½•")
                    return df
                else:
                    print(f"âš ï¸  [DefiLlama] å“åº”ä¸­æ— æ•°æ®å­—æ®µ")
            else:
                print(f"âŒ [DefiLlama] HTTPé”™è¯¯: {response.status_code}")
        
        except requests.exceptions.SSLError as e:
            print(f"âŒ [DefiLlama] SSLé”™è¯¯: è¯·å°è¯•ç¦ç”¨SSLéªŒè¯")
        except requests.exceptions.Timeout as e:
            print(f"âŒ [DefiLlama] è¶…æ—¶é”™è¯¯: {REQUEST_TIMEOUT}s")
        except Exception as e:
            print(f"âŒ [DefiLlama] æŸ¥è¯¢å¤±è´¥: {str(e)[:100]}...")
        
        return pd.DataFrame()
    
    def build_historical_database(self, pool_name: str = TARGET_POOL, days_to_collect: int = CURRENT_DAYS_SETTING):
        """
        æ–¹æ³•3: è‡ªå»ºå…è´¹å†å²æ•°æ®åº“ (ä¼˜åŒ–ç‰ˆ - é¿å…æ— é™å¾ªç¯)
        é€šè¿‡æœ‰é™æ¬¡æ•°å°è¯•è·å–å®æ—¶æ•°æ®ï¼Œç„¶åç”Ÿæˆåˆæˆå†å²æ•°æ®
        """
        
        if not ENABLE_SELF_BUILT:
            print(f"âš ï¸  è‡ªå»ºæ•°æ®åº“å·²è¢«ç¦ç”¨")
            return pd.DataFrame()
        
        print(f"ğŸ—ï¸  å¼€å§‹è‡ªå»º {pool_name} å†å²æ•°æ®åº“ ({days_to_collect} å¤©)...")
        
        try:
            from real_data_collector import CurveRealDataCollector
            from data_manager import CurveDataManager
            
            collector = CurveRealDataCollector()
            manager = CurveDataManager(str(self.cache_dir / "self_built"))
            
            # ğŸ”¥ é™åˆ¶å°è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
            max_attempts = min(MAX_COLLECTION_ATTEMPTS, days_to_collect)
            successful_attempts = 0
            base_data = None
            
            print(f"ğŸ”„ å°è¯•è·å–åŸºç¡€æ•°æ® (æœ€å¤š {max_attempts} æ¬¡)...")
            
            # å…ˆå°è¯•è·å–ä¸€æ¬¡æœ‰æ•ˆçš„å®æ—¶æ•°æ®ä½œä¸ºåŸºå‡†
            for attempt in range(max_attempts):
                try:
                    pool_data = collector.get_real_time_data(pool_name)
                    if pool_data:
                        base_data = pool_data
                        print(f"âœ… ç¬¬ {attempt + 1} æ¬¡å°è¯•æˆåŠŸè·å–åŸºç¡€æ•°æ®")
                        break
                    
                    if attempt % 10 == 0 and attempt > 0:
                        print(f"âš ï¸  å·²å°è¯• {attempt + 1} æ¬¡ï¼Œç»§ç»­é‡è¯•...")
                    
                    time.sleep(REQUEST_RETRY_DELAY)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    
                except Exception as e:
                    if attempt % 10 == 0:
                        print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)[:50]}...")
                    continue
            
            # å¦‚æœæ— æ³•è·å–çœŸå®æ•°æ®ï¼Œç”Ÿæˆåˆæˆæ•°æ®
            if not base_data:
                print("âš ï¸  æ— æ³•è·å–çœŸå®æ•°æ®ï¼Œç”Ÿæˆåˆæˆå†å²æ•°æ®...")
                return self._generate_synthetic_data(pool_name, days_to_collect)
            
            # åŸºäºçœŸå®æ•°æ®ç”Ÿæˆå†å²æ•°æ®
            print(f"ğŸ“Š åŸºäºçœŸå®æ•°æ®ç”Ÿæˆ {days_to_collect} å¤©å†å²æ•°æ®...")
            simulated_records = []
            
            for day in range(days_to_collect):
                # æ¯å¤©ç”Ÿæˆå‡ ä¸ªæ•°æ®ç‚¹è€Œä¸æ˜¯æ¯å°æ—¶
                points_per_day = 4  # æ¯6å°æ—¶ä¸€ä¸ªæ•°æ®ç‚¹
                
                for point in range(points_per_day):
                    hour_offset = day * 24 + point * 6
                    
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹æ·»åŠ éšæœºæ³¢åŠ¨
                    noise_factor = 0.02  # 2%çš„éšæœºæ³¢åŠ¨
                    
                    record = {
                        'timestamp': datetime.now() - timedelta(hours=hour_offset),
                        'pool_address': base_data.pool_address,
                        'pool_name': base_data.pool_name,
                        'virtual_price': base_data.virtual_price * (1 + np.random.normal(0, noise_factor)),
                        'volume_24h': base_data.volume_24h * (1 + np.random.normal(0, noise_factor * 2)),
                        'apy': max(0, base_data.apy * (1 + np.random.normal(0, noise_factor))),
                        'total_supply': base_data.total_supply * (1 + np.random.normal(0, noise_factor * 0.5))
                    }
                    
                    # æ·»åŠ ä»£å¸ä½™é¢
                    for i, (token, balance) in enumerate(zip(base_data.tokens, base_data.balances)):
                        record[f'{token.lower()}_balance'] = balance * (1 + np.random.normal(0, noise_factor))
                    
                    simulated_records.append(record)
                
                # æ˜¾ç¤ºè¿›åº¦
                if day % max(1, days_to_collect // 10) == 0:
                    progress = (day / days_to_collect) * 100
                    print(f"ğŸ“ˆ ç”Ÿæˆè¿›åº¦: {progress:.0f}% ({day}/{days_to_collect} å¤©)")
        
        except ImportError as e:
            print(f"âŒ å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
            print("ğŸ’¡ ç”ŸæˆåŸºç¡€åˆæˆæ•°æ®...")
            return self._generate_synthetic_data(pool_name, days_to_collect)
        except Exception as e:
            print(f"âŒ è‡ªå»ºæ•°æ®åº“å¤±è´¥: {e}")
            print("ğŸ’¡ fallbackåˆ°åˆæˆæ•°æ®...")
            return self._generate_synthetic_data(pool_name, days_to_collect)
        
        if simulated_records:
            df = pd.DataFrame(simulated_records)
            
            # ä¿å­˜è‡ªå»ºå†å²æ•°æ®
            filename = f"{pool_name}_self_built_historical_{days_to_collect}d.csv"
            filepath = self.cache_dir / filename
            df.to_csv(filepath, index=False, encoding='utf-8')
            
            print(f"âœ… è‡ªå»ºå†å²æ•°æ®åº“å®Œæˆ: {filepath}")
            print(f"ğŸ“Š æ€»è®¡ {len(df)} æ¡è®°å½•ï¼Œæ—¶é—´è·¨åº¦ {days_to_collect} å¤©")
            
            return df
        
        return pd.DataFrame()

    def _generate_synthetic_data(self, pool_name: str, days: int) -> pd.DataFrame:
        """ç”Ÿæˆåˆæˆå†å²æ•°æ® - å½“æ‰€æœ‰çœŸå®æ•°æ®æºéƒ½å¤±è´¥æ—¶ä½¿ç”¨"""
        
        print(f"ğŸ­ ä¸º {pool_name} ç”Ÿæˆ {days} å¤©åˆæˆå†å²æ•°æ®...")
        
        # æ ¹æ®æ± å­ç±»å‹è®¾ç½®ä¸åŒçš„åŸºç¡€å‚æ•°
        pool_configs = {
            'mim': {
                'tokens': ['MIM', 'USDC', 'USDT'], 
                'base_balance': 1000000,
                'base_apy': 0.05,
                'base_volume': 500000
            },
            '3pool': {
                'tokens': ['USDC', 'USDT', 'DAI'],
                'base_balance': 5000000, 
                'base_apy': 0.03,
                'base_volume': 2000000
            },
            'frax': {
                'tokens': ['FRAX', 'USDC'],
                'base_balance': 800000,
                'base_apy': 0.06,
                'base_volume': 300000
            },
            'lusd': {
                'tokens': ['LUSD', 'USDC', 'USDT'],
                'base_balance': 600000,
                'base_apy': 0.04,
                'base_volume': 200000
            }
        }
        
        config = pool_configs.get(pool_name, pool_configs['3pool'])
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—
        dates = pd.date_range(
            end=datetime.now(),
            periods=days * 6,  # æ¯å¤©6ä¸ªæ•°æ®ç‚¹
            freq='4H'  # æ¯4å°æ—¶ä¸€ä¸ªç‚¹
        )
        
        records = []
        for i, timestamp in enumerate(dates):
            # æ·»åŠ ä¸€äº›è¶‹åŠ¿å’Œéšæœºæ€§
            trend_factor = 1 + 0.1 * np.sin(i / (days * 0.5))  # é•¿æœŸè¶‹åŠ¿
            noise_factor = np.random.normal(1, 0.02)  # éšæœºæ³¢åŠ¨
            
            record = {
                'timestamp': timestamp,
                'pool_address': AVAILABLE_POOLS[pool_name]['address'],
                'pool_name': AVAILABLE_POOLS[pool_name]['name'],
                'virtual_price': 1.0 * trend_factor * noise_factor,
                'volume_24h': config['base_volume'] * trend_factor * noise_factor,
                'apy': max(0, config['base_apy'] * trend_factor * noise_factor),
                'total_supply': config['base_balance'] * 3 * trend_factor
            }
            
            # æ·»åŠ ä»£å¸ä½™é¢
            for j, token in enumerate(config['tokens']):
                balance_variation = np.random.normal(1, 0.05)
                record[f'{token.lower()}_balance'] = config['base_balance'] * balance_variation
            
            records.append(record)
        
        df = pd.DataFrame(records)
        
        # ä¿å­˜åˆæˆæ•°æ®
        filename = f"{pool_name}_synthetic_historical_{days}d.csv"  
        filepath = self.cache_dir / filename
        df.to_csv(filepath, index=False, encoding='utf-8')
        
        print(f"âœ… åˆæˆæ•°æ®ç”Ÿæˆå®Œæˆ: {filepath}")
        print(f"ğŸ“Š ç”Ÿæˆäº† {len(df)} æ¡åˆæˆè®°å½•")
        
        return df
    
    def get_comprehensive_free_data(self, pool_address: str = TARGET_POOL_ADDRESS, pool_name: str = TARGET_POOL, days: int = CURRENT_DAYS_SETTING) -> pd.DataFrame:
        """
        æ–¹æ³•4: ç»¼åˆå…è´¹æ•°æ®ç­–ç•¥ (ä¼˜åŒ–ç‰ˆ)
        ç»“åˆå¤šä¸ªå…è´¹æºè·å–æœ€å®Œæ•´çš„å†å²æ•°æ®ï¼ŒåŒ…å«fallbackæœºåˆ¶
        """
        
        print(f"ğŸ”„ ç»¼åˆå…è´¹ç­–ç•¥è·å– {pool_name} å†å²æ•°æ® ({days} å¤©)...")
        
        all_data = []
        data_sources_tried = []
        
        # 1. å°è¯•The Graph (å¦‚æœå¯ç”¨)
        if ENABLE_THEGRAPH_API:
            try:
                thegraph_data = self.get_thegraph_historical_data(pool_address, days)
                if not thegraph_data.empty:
                    thegraph_data['source'] = 'thegraph'
                    all_data.append(thegraph_data)
                    print(f"âœ… The Graph: {len(thegraph_data)} æ¡è®°å½•")
                data_sources_tried.append('The Graph')
            except Exception as e:
                print(f"âš ï¸  The Graphå°è¯•å¤±è´¥: {str(e)[:50]}...")
                data_sources_tried.append('The Graph (å¤±è´¥)')
        else:
            print("âš ï¸  The Graphå·²ç¦ç”¨")
        
        # 2. å°è¯•DefiLlama APY (å¦‚æœå¯ç”¨)
        if ENABLE_DEFILLAMA:
            try:
                defillama_data = self.get_defillama_apy_history(pool_address)
                if not defillama_data.empty:
                    defillama_data['source'] = 'defillama'
                    all_data.append(defillama_data)
                    print(f"âœ… DefiLlama: {len(defillama_data)} æ¡è®°å½•")
                data_sources_tried.append('DefiLlama')
            except Exception as e:
                print(f"âš ï¸  DefiLlamaå°è¯•å¤±è´¥: {str(e)[:50]}...")
                data_sources_tried.append('DefiLlama (å¤±è´¥)')
        
        # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªå»ºæ•°æ®åº“è¡¥å……
        total_records = sum(len(df) for df in all_data) if all_data else 0
        min_required_records = max(days // 10, 5)  # è‡³å°‘éœ€è¦çš„è®°å½•æ•°
        
        if total_records < min_required_records:
            print(f"ğŸ“Š å…è´¹APIæ•°æ®ä¸è¶³ ({total_records} < {min_required_records})ï¼Œå¯ç”¨è‡ªå»ºå†å²æ•°æ®åº“...")
            
            if ENABLE_SELF_BUILT:
                try:
                    self_built_data = self.build_historical_database(pool_name, days)
                    if not self_built_data.empty:
                        self_built_data['source'] = 'self_built'
                        all_data.append(self_built_data)
                        print(f"âœ… è‡ªå»ºæ•°æ®: {len(self_built_data)} æ¡è®°å½•")
                    data_sources_tried.append('è‡ªå»ºæ•°æ®åº“')
                except Exception as e:
                    print(f"âš ï¸  è‡ªå»ºæ•°æ®åº“å¤±è´¥: {str(e)[:50]}...")
                    data_sources_tried.append('è‡ªå»ºæ•°æ®åº“ (å¤±è´¥)')
            else:
                print("âš ï¸  è‡ªå»ºæ•°æ®åº“å·²ç¦ç”¨")
        
        # 4. åˆå¹¶æ‰€æœ‰æ•°æ®æº
        if all_data:
            try:
                # æŒ‰æ—¶é—´æˆ³åˆå¹¶æ•°æ®
                combined_df = pd.concat(all_data, ignore_index=True)
                
                # ç¡®ä¿æ—¶é—´æˆ³åˆ—å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
                if 'timestamp' not in combined_df.columns:
                    print("âš ï¸  æ•°æ®ä¸­ç¼ºå°‘timestampåˆ—ï¼Œæ·»åŠ é»˜è®¤æ—¶é—´æˆ³")
                    combined_df['timestamp'] = pd.date_range(
                        end=datetime.now(),
                        periods=len(combined_df),
                        freq='H'
                    )
                
                combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)
                
                # å»é‡ (ä¿ç•™æœ€æ–°çš„è®°å½•)
                if len(combined_df) > 1:
                    combined_df = combined_df.drop_duplicates(subset=['timestamp'], keep='last')
                
                # ä¿å­˜ç»¼åˆæ•°æ®
                filename = f"{pool_name}_comprehensive_free_historical_{days}d.csv"
                filepath = self.cache_dir / filename
                combined_df.to_csv(filepath, index=False, encoding='utf-8')
                
                print(f"ğŸ‰ ç»¼åˆå…è´¹å†å²æ•°æ®è·å–å®Œæˆ!")
                print(f"ğŸ“ ä¿å­˜ä½ç½®: {filepath}")
                print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)}")
                print(f"ğŸ”„ æ•°æ®æ¥æº: {', '.join([df['source'].iloc[0] for df in all_data if 'source' in df.columns and len(df) > 0])}")
                
                if 'timestamp' in combined_df.columns and len(combined_df) > 0:
                    print(f"ğŸ—“ï¸  æ—¶é—´èŒƒå›´: {combined_df['timestamp'].min()} åˆ° {combined_df['timestamp'].max()}")
                
                return combined_df
            
            except Exception as e:
                print(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {e}")
                # è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®æº
                if all_data:
                    print(f"ğŸ’¡ è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨æ•°æ®æº ({len(all_data[0])} æ¡è®°å½•)")
                    return all_data[0]
        
        print(f"âŒ æ‰€æœ‰å…è´¹æ•°æ®æºéƒ½å¤±è´¥")
        print(f"ğŸ” å·²å°è¯•çš„æ•°æ®æº: {', '.join(data_sources_tried)}")
        print(f"ğŸ’¡ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¯ç”¨SSLéªŒè¯")
        
        return pd.DataFrame()
    
    def setup_daily_collection(self, pool_name: str = TARGET_POOL):
        """
        æ–¹æ³•5: è®¾ç½®æ¯æ—¥æ•°æ®æ”¶é›†ä»»åŠ¡ (é•¿æœŸå…è´¹æ–¹æ¡ˆ)
        å»ºè®®ä½¿ç”¨cronå®šæ—¶ä»»åŠ¡æ¯å¤©è¿è¡Œ
        """
        
        print(f"â° è®¾ç½® {pool_name} æ¯æ—¥æ•°æ®æ”¶é›†...")
        
        # åˆ›å»ºæ¯æ—¥æ”¶é›†è„šæœ¬
        script_content = f"""#!/usr/bin/env python3
# æ¯æ—¥æ•°æ®æ”¶é›†è„šæœ¬ - ç”±cronå®šæ—¶è¿è¡Œ
import sys
sys.path.append('/path/to/your/Quantum_curve_predict')

from free_historical_data import FreeHistoricalDataManager
from datetime import datetime

def daily_collect():
    manager = FreeHistoricalDataManager()
    
    # è·å–ä»Šæ—¥æ•°æ®
    df = manager.get_thegraph_historical_data('0xbebc44782c7db0a1a60cb6fe97d0b483032ff1c7', days=1)
    
    if not df.empty:
        # è¿½åŠ åˆ°å†å²æ•°æ®æ–‡ä»¶
        filename = 'daily_collection_{pool_name}.csv'
        filepath = manager.cache_dir / filename
        
        if filepath.exists():
            existing_df = pd.read_csv(filepath)
            combined_df = pd.concat([existing_df, df], ignore_index=True)
        else:
            combined_df = df
        
        combined_df.to_csv(filepath, index=False, encoding='utf-8')
        print(f"âœ… {{datetime.now()}}: æ¯æ—¥æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±{{len(combined_df)}}æ¡è®°å½•")
    else:
        print(f"âŒ {{datetime.now()}}: ä»Šæ—¥æ•°æ®æ”¶é›†å¤±è´¥")

if __name__ == "__main__":
    daily_collect()
"""
        
        script_file = self.cache_dir / f"daily_collect_{pool_name}.py"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print(f"ğŸ“ æ¯æ—¥æ”¶é›†è„šæœ¬å·²åˆ›å»º: {script_file}")
        print("ğŸ’¡ è®¾ç½®cronå®šæ—¶ä»»åŠ¡:")
        print(f"   0 1 * * * python3 {script_file.absolute()}")
        print("   (æ¯å¤©å‡Œæ™¨1ç‚¹è¿è¡Œ)")

    def get_batch_historical_data(self, pools_dict: dict, days: int = CURRENT_DAYS_SETTING, 
                                 max_concurrent: int = 3, delay_between_batches: int = 2) -> dict:
        """
        æ‰¹é‡è·å–å¤šä¸ªæ± å­çš„å†å²æ•°æ®
        
        Args:
            pools_dict: æ± å­å­—å…¸ (æ¥è‡ª get_pools_by_priority ç­‰å‡½æ•°)
            days: è·å–å¤©æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°é‡
            delay_between_batches: æ‰¹æ¬¡é—´å»¶è¿Ÿ(ç§’)
        
        Returns:
            {pool_name: DataFrame} å­—å…¸
        """
        
        print(f"ğŸš€ æ‰¹é‡è·å– {len(pools_dict)} ä¸ªæ± å­çš„ {days} å¤©å†å²æ•°æ®...")
        print(f"ğŸ“‹ æ± å­åˆ—è¡¨: {', '.join(pools_dict.keys())}")
        print("="*60)
        
        results = {}
        successful = 0
        failed = 0
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæ± å­
        sorted_pools = sorted(pools_dict.items(), key=lambda x: x[1]['priority'])
        
        # åˆ†æ‰¹å¤„ç†é¿å…APIé™åˆ¶
        import math
        total_batches = math.ceil(len(sorted_pools) / max_concurrent)
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * max_concurrent
            end_idx = min(start_idx + max_concurrent, len(sorted_pools))
            current_batch = sorted_pools[start_idx:end_idx]
            
            print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            print(f"   æ± å­: {[pool[0] for pool in current_batch]}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for pool_name, pool_info in current_batch:
                try:
                    print(f"  ğŸ”„ [{pool_name}] {pool_info['name']} (ä¼˜å…ˆçº§:{pool_info['priority']})")
                    
                    # æ£€æŸ¥ç¼“å­˜
                    cache_file = self.cache_dir / f"{pool_name}_batch_historical_{days}d.csv"
                    if cache_file.exists():
                        try:
                            df = pd.read_csv(cache_file)
                            df['timestamp'] = pd.to_datetime(df['timestamp'])
                            print(f"  âœ… [{pool_name}] ä»ç¼“å­˜åŠ è½½ {len(df)} æ¡è®°å½•")
                            results[pool_name] = df
                            successful += 1
                            continue
                        except Exception as e:
                            print(f"  âš ï¸  [{pool_name}] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
                    
                    # è·å–æ–°æ•°æ®  
                    df = self.get_comprehensive_free_data(
                        pool_info['address'], 
                        pool_name, 
                        days=days
                    )
                    
                    if not df.empty:
                        # æ·»åŠ æ± å­ä¿¡æ¯
                        df['pool_name'] = pool_name
                        df['pool_type'] = pool_info['type']
                        df['priority'] = pool_info['priority']
                        
                        # ä¿å­˜åˆ°ç¼“å­˜
                        df.to_csv(cache_file, index=False)
                        
                        results[pool_name] = df
                        successful += 1
                        print(f"  âœ… [{pool_name}] è·å–æˆåŠŸ: {len(df)} æ¡è®°å½•")
                        
                        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡  
                        if 'virtual_price' in df.columns:
                            latest_vp = df['virtual_price'].iloc[-1] if len(df) > 0 else 0
                            print(f"     Virtual Price: {latest_vp:.6f}")
                            
                    else:
                        print(f"  âŒ [{pool_name}] æ²¡æœ‰è·å–åˆ°æ•°æ®")
                        failed += 1
                        results[pool_name] = pd.DataFrame()
                        
                except Exception as e:
                    print(f"  âŒ [{pool_name}] è·å–å¤±è´¥: {str(e)[:100]}...")
                    failed += 1
                    results[pool_name] = pd.DataFrame()
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_idx < total_batches - 1:  # ä¸æ˜¯æœ€åä¸€æ‰¹æ¬¡
                print(f"  â³ ç­‰å¾… {delay_between_batches} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
                time.sleep(delay_between_batches)
        
        print("\n" + "="*60)
        print(f"ğŸ“Š æ‰¹é‡è·å–å®Œæˆ!")
        print(f"   âœ… æˆåŠŸ: {successful}/{len(pools_dict)}")
        print(f"   âŒ å¤±è´¥: {failed}/{len(pools_dict)}")
        print(f"   æˆåŠŸç‡: {successful/len(pools_dict)*100:.1f}%")
        
        return results

    def get_all_main_pools_data(self, days: int = CURRENT_DAYS_SETTING) -> dict:
        """
        è·å–æ‰€æœ‰ä¸»è¦æ± å­æ•°æ® (ä¼˜å…ˆçº§ 1-3)
        
        Returns:
            {pool_name: DataFrame} å­—å…¸
        """
        main_pools = get_all_main_pools()
        print(f"ğŸ¯ è·å– {len(main_pools)} ä¸ªä¸»è¦æ± å­æ•°æ®...")
        
        return self.get_batch_historical_data(main_pools, days)

    def get_high_priority_pools_data(self, days: int = CURRENT_DAYS_SETTING) -> dict:
        """
        è·å–é«˜ä¼˜å…ˆçº§æ± å­æ•°æ® (ä¼˜å…ˆçº§ 1-2)
        
        Returns:
            {pool_name: DataFrame} å­—å…¸  
        """
        high_priority_pools = get_high_priority_pools()
        print(f"â­ è·å– {len(high_priority_pools)} ä¸ªé«˜ä¼˜å…ˆçº§æ± å­æ•°æ®...")
        
        return self.get_batch_historical_data(high_priority_pools, days)

    def get_stable_pools_data(self, days: int = CURRENT_DAYS_SETTING) -> dict:
        """
        è·å–æ‰€æœ‰ç¨³å®šå¸æ± æ•°æ®
        
        Returns:
            {pool_name: DataFrame} å­—å…¸
        """
        stable_pools = get_stable_pools()
        print(f"ğŸ’° è·å– {len(stable_pools)} ä¸ªç¨³å®šå¸æ± æ•°æ®...")
        
        return self.get_batch_historical_data(stable_pools, days)

    def get_all_pools_data(self, days: int = CURRENT_DAYS_SETTING, skip_low_priority: bool = True) -> dict:
        """
        è·å–æ‰€æœ‰æ± å­æ•°æ® (å¯é€‰æ‹©è·³è¿‡ä½ä¼˜å…ˆçº§)
        
        Args:
            days: è·å–å¤©æ•°  
            skip_low_priority: æ˜¯å¦è·³è¿‡ä¼˜å…ˆçº§5çš„æ± å­
            
        Returns:
            {pool_name: DataFrame} å­—å…¸
        """
        if skip_low_priority:
            pools = get_pools_by_priority(min_priority=1, max_priority=4)
            print(f"ğŸŒ è·å–æ‰€æœ‰æ± å­æ•°æ® (è·³è¿‡ä½ä¼˜å…ˆçº§): {len(pools)} ä¸ªæ± å­")
        else:
            pools = AVAILABLE_POOLS
            print(f"ğŸŒ è·å–æ‰€æœ‰æ± å­æ•°æ® (åŒ…å«å…¨éƒ¨): {len(pools)} ä¸ªæ± å­")
        
        return self.get_batch_historical_data(pools, days, max_concurrent=2, delay_between_batches=3)

    def get_pools_by_type_data(self, pool_type: str, days: int = CURRENT_DAYS_SETTING) -> dict:
        """
        æŒ‰æ± å­ç±»å‹è·å–æ•°æ®
        
        Args:
            pool_type: æ± å­ç±»å‹ï¼Œå¦‚ 'stable', 'metapool', 'eth_pool', 'btc_pool', 'crypto' ç­‰
            days: è·å–å¤©æ•°
            
        Returns:
            {pool_name: DataFrame} å­—å…¸
        """
        pools = get_pools_by_priority(pool_types=[pool_type])
        print(f"ğŸ·ï¸  è·å– {pool_type} ç±»å‹æ± å­æ•°æ®: {len(pools)} ä¸ªæ± å­")
        
        return self.get_batch_historical_data(pools, days)

    def export_batch_data_to_excel(self, batch_data: dict, filename: str = None) -> str:
        """
        å°†æ‰¹é‡æ•°æ®å¯¼å‡ºåˆ°Excelæ–‡ä»¶
        
        Args:
            batch_data: æ¥è‡ªæ‰¹é‡è·å–å‡½æ•°çš„æ•°æ®å­—å…¸
            filename: è¾“å‡ºæ–‡ä»¶å (å¯é€‰)
            
        Returns:
            ç”Ÿæˆçš„Excelæ–‡ä»¶è·¯å¾„
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"curve_pools_data_{timestamp}.xlsx"
        
        output_path = self.cache_dir / filename
        
        try:
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                
                # åˆ›å»ºæ±‡æ€»è¡¨
                summary_data = []
                for pool_name, df in batch_data.items():
                    if not df.empty:
                        summary_data.append({
                            'Pool Name': pool_name,
                            'Pool Type': df['pool_type'].iloc[0] if 'pool_type' in df.columns else 'Unknown',
                            'Priority': df['priority'].iloc[0] if 'priority' in df.columns else 'Unknown',
                            'Records Count': len(df),
                            'Date Range': f"{df['timestamp'].min().date()} to {df['timestamp'].max().date()}" if len(df) > 0 else 'No data',
                            'Has Virtual Price': 'virtual_price' in df.columns,
                            'Has Volume': 'volume_24h' in df.columns,
                            'Data Sources': ', '.join(df['source'].unique()) if 'source' in df.columns else 'Unknown'
                        })
                
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                # ä¸ºæ¯ä¸ªæ± å­åˆ›å»ºå•ç‹¬çš„å·¥ä½œè¡¨
                for pool_name, df in batch_data.items():
                    if not df.empty:
                        # é™åˆ¶å·¥ä½œè¡¨åç§°é•¿åº¦
                        sheet_name = pool_name[:31] if len(pool_name) <= 31 else pool_name[:28] + '...'
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                
            print(f"ğŸ“„ æ‰¹é‡æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ Excelå¯¼å‡ºå¤±è´¥: {e}")
            return ""

    def analyze_batch_data(self, batch_data: dict) -> pd.DataFrame:
        """
        åˆ†ææ‰¹é‡æ•°æ®ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            batch_data: æ¥è‡ªæ‰¹é‡è·å–å‡½æ•°çš„æ•°æ®å­—å…¸
            
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„DataFrame
        """
        analysis_results = []
        
        for pool_name, df in batch_data.items():
            pool_info = AVAILABLE_POOLS.get(pool_name, {})
            
            if df.empty:
                analysis_results.append({
                    'Pool': pool_name,
                    'Type': pool_info.get('type', 'Unknown'),
                    'Priority': pool_info.get('priority', 'Unknown'),
                    'Status': 'No Data',
                    'Records': 0,
                    'Days Coverage': 0,
                    'Avg Virtual Price': None,
                    'Avg Volume 24h': None,
                    'Last Update': None
                })
                continue
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            days_coverage = (df['timestamp'].max() - df['timestamp'].min()).days if len(df) > 1 else 0
            
            analysis_results.append({
                'Pool': pool_name,
                'Type': pool_info.get('type', 'Unknown'),
                'Priority': pool_info.get('priority', 'Unknown'), 
                'Status': 'Success',
                'Records': len(df),
                'Days Coverage': days_coverage,
                'Avg Virtual Price': df['virtual_price'].mean() if 'virtual_price' in df.columns else None,
                'Avg Volume 24h': df['volume_24h'].mean() if 'volume_24h' in df.columns else None,
                'Last Update': df['timestamp'].max() if len(df) > 0 else None
            })
        
        analysis_df = pd.DataFrame(analysis_results)
        
        print("\nğŸ“ˆ æ‰¹é‡æ•°æ®åˆ†ææŠ¥å‘Š:")
        print("="*60)
        print(analysis_df.to_string(index=False))
        
        return analysis_df

def demo_free_historical_data():
    """æ¼”ç¤ºå…è´¹å†å²æ•°æ®è·å– - ä¼˜åŒ–ç‰ˆ"""
    
    print("ğŸ†“ å…è´¹å†å²æ•°æ®è·å–æ¼”ç¤º - æ‰©å±•ç‰ˆ")
    print("=" * 60)
    
    manager = FreeHistoricalDataManager()
    
    print("ğŸ“‹ å¯ç”¨çš„æ± å­é…ç½®:")
    print(f"   æ€»è®¡: {len(AVAILABLE_POOLS)} ä¸ªæ± å­")
    print(f"   é«˜ä¼˜å…ˆçº§ (1-2): {len(get_high_priority_pools())} ä¸ª")
    print(f"   ä¸»è¦æ± å­ (1-3): {len(get_all_main_pools())} ä¸ª")  
    print(f"   ç¨³å®šå¸æ± : {len(get_stable_pools())} ä¸ª")
    print()
    
    # å±•ç¤ºä¸åŒç±»å‹çš„æ± å­
    print("ğŸ·ï¸  æ± å­åˆ†ç±»:")
    pool_types = set(pool['type'] for pool in AVAILABLE_POOLS.values())
    for pool_type in sorted(pool_types):
        pools_of_type = [name for name, info in AVAILABLE_POOLS.items() if info['type'] == pool_type]
        print(f"   {pool_type}: {len(pools_of_type)} ä¸ª ({', '.join(pools_of_type[:3])}...)")
    print()

    # æ¼”ç¤º1: å•ä¸ªæ± å­æ•°æ®è·å– (ä¿æŒåŸæœ‰æ¼”ç¤º)
    print("=" * 60)
    print("ğŸ¯ æ¼”ç¤º1: å•ä¸ªæ± å­å†å²æ•°æ®è·å–")
    print("=" * 60)
    
    single_pool = TARGET_POOL
    print(f"è·å– {single_pool} çš„ {CURRENT_DAYS_SETTING} å¤©å†å²æ•°æ®...")
    
    df_single = manager.get_comprehensive_free_data(
        AVAILABLE_POOLS[single_pool]['address'], 
        single_pool, 
        days=CURRENT_DAYS_SETTING
    )
    
    if not df_single.empty:
        print(f"âœ… å•ä¸ªæ± å­æ•°æ®è·å–æˆåŠŸ: {len(df_single)} æ¡è®°å½•")
        print(f"   æ—¶é—´è·¨åº¦: {(df_single['timestamp'].max() - df_single['timestamp'].min()).days} å¤©")
        if 'virtual_price' in df_single.columns:
            print(f"   Virtual Price èŒƒå›´: {df_single['virtual_price'].min():.6f} - {df_single['virtual_price'].max():.6f}")
    else:
        print("âŒ å•ä¸ªæ± å­æ•°æ®è·å–å¤±è´¥")
    
    print()
    
    # æ¼”ç¤º2: é«˜ä¼˜å…ˆçº§æ± å­æ‰¹é‡è·å–
    print("=" * 60)
    print("â­ æ¼”ç¤º2: é«˜ä¼˜å…ˆçº§æ± å­æ‰¹é‡æ•°æ®è·å–")  
    print("=" * 60)
    
    batch_data_high = manager.get_high_priority_pools_data(days=CURRENT_DAYS_SETTING)
    
    if batch_data_high:
        print(f"\nğŸ“Š é«˜ä¼˜å…ˆçº§æ‰¹é‡æ•°æ®è·å–ç»“æœ:")
        total_records = sum(len(df) for df in batch_data_high.values() if not df.empty)
        successful_pools = sum(1 for df in batch_data_high.values() if not df.empty)
        print(f"   æˆåŠŸè·å–: {successful_pools}/{len(batch_data_high)} ä¸ªæ± å­")
        print(f"   æ€»è®°å½•æ•°: {total_records}")
        
        # ç®€è¦åˆ†æ
        analysis = manager.analyze_batch_data(batch_data_high)
        
    print()
    
    # æ¼”ç¤º3: æŒ‰ç±»å‹è·å–æ•°æ®
    print("=" * 60)
    print("ğŸ·ï¸  æ¼”ç¤º3: æŒ‰ç±»å‹è·å–æ•°æ® - ç¨³å®šå¸æ± ")
    print("=" * 60)
    
    stable_data = manager.get_pools_by_type_data('stable', days=CURRENT_DAYS_SETTING)
    
    if stable_data:
        stable_successful = sum(1 for df in stable_data.values() if not df.empty)
        print(f"âœ… ç¨³å®šå¸æ± æ•°æ®è·å–: {stable_successful}/{len(stable_data)} ä¸ªæˆåŠŸ")
        
    print()
    
    # æ¼”ç¤º4: Excelå¯¼å‡º
    print("=" * 60)
    print("ğŸ“„ æ¼”ç¤º4: æ‰¹é‡æ•°æ®å¯¼å‡ºåˆ°Excel")  
    print("=" * 60)
    
    if batch_data_high:
        excel_path = manager.export_batch_data_to_excel(
            batch_data_high, 
            f"curve_high_priority_pools_{CURRENT_DAYS_SETTING}d.xlsx"
        )
        if excel_path:
            print(f"âœ… Excelæ–‡ä»¶å¯¼å‡ºæˆåŠŸ: {excel_path}")
        
    print()
    
    print("=" * 60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ! ä¸»è¦åŠŸèƒ½å·²éªŒè¯:")
    print("   âœ… å•ä¸ªæ± å­æ•°æ®è·å–")  
    print("   âœ… æ‰¹é‡æ•°æ®è·å–")
    print("   âœ… æŒ‰ä¼˜å…ˆçº§ç­›é€‰")
    print("   âœ… æŒ‰ç±»å‹ç­›é€‰")
    print("   âœ… æ•°æ®åˆ†æ")
    print("   âœ… Excelå¯¼å‡º")
    print("=" * 60)

def demo_batch_collection_scenarios():
    """æ¼”ç¤ºå„ç§æ‰¹é‡è·å–åœºæ™¯"""
    
    print("\nğŸš€ æ‰¹é‡æ•°æ®è·å–åœºæ™¯æ¼”ç¤º")
    print("=" * 60)
    
    manager = FreeHistoricalDataManager()
    
    # åœºæ™¯1: å¿«é€Ÿè·å–ä¸»è¦æ± å­æ•°æ®
    print("ğŸ“ˆ åœºæ™¯1: ä¸»è¦æ± å­å¿«é€Ÿæ•°æ®è·å–")
    print("-" * 40)
    main_pools_data = manager.get_all_main_pools_data(days=CURRENT_DAYS_SETTING)
    print(f"âœ… ä¸»è¦æ± å­æ•°æ®è·å–å®Œæˆ: {len([d for d in main_pools_data.values() if not d.empty])}/{len(main_pools_data)} ä¸ªæˆåŠŸ")
    print()
    
    # åœºæ™¯2: ETHç›¸å…³æ± å­
    print("ğŸ”¥ åœºæ™¯2: ETHç›¸å…³æ± å­æ•°æ®è·å–") 
    print("-" * 40)
    eth_pools_data = manager.get_pools_by_type_data('eth_pool', days=CURRENT_DAYS_SETTING)
    if eth_pools_data:
        print(f"âœ… ETHæ± æ•°æ®è·å–å®Œæˆ: {len([d for d in eth_pools_data.values() if not d.empty])}/{len(eth_pools_data)} ä¸ªæˆåŠŸ")
    print()
    
    # åœºæ™¯3: BTCç›¸å…³æ± å­
    print("â‚¿ åœºæ™¯3: BTCç›¸å…³æ± å­æ•°æ®è·å–")
    print("-" * 40) 
    btc_pools_data = manager.get_pools_by_type_data('btc_pool', days=CURRENT_DAYS_SETTING)
    if btc_pools_data:
        print(f"âœ… BTCæ± æ•°æ®è·å–å®Œæˆ: {len([d for d in btc_pools_data.values() if not d.empty])}/{len(btc_pools_data)} ä¸ªæˆåŠŸ")
    print()
    
    # åœºæ™¯4: ç»¼åˆæ¯”è¾ƒåˆ†æ
    print("ğŸ“Š åœºæ™¯4: ç»¼åˆæ•°æ®æ¯”è¾ƒåˆ†æ")
    print("-" * 40)
    
    all_batch_data = {}
    all_batch_data.update(main_pools_data)
    if eth_pools_data:
        all_batch_data.update(eth_pools_data)  
    if btc_pools_data:
        all_batch_data.update(btc_pools_data)
    
    if all_batch_data:
        analysis = manager.analyze_batch_data(all_batch_data)
        
        # å¯¼å‡ºç»¼åˆæ•°æ®
        excel_path = manager.export_batch_data_to_excel(
            all_batch_data,
            f"curve_comprehensive_pools_{CURRENT_DAYS_SETTING}d.xlsx"
        )
        print(f"âœ… ç»¼åˆæ•°æ®å·²å¯¼å‡º: {excel_path}")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ‰¹é‡è·å–åœºæ™¯æ¼”ç¤ºå®Œæˆ!")
    print("   å¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨ä¸åŒçš„è·å–ç­–ç•¥")
    print("=" * 60)

def show_available_pools_info():
    """æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ± å­çš„è¯¦ç»†ä¿¡æ¯"""
    
    print("\nğŸ“‹ å¯ç”¨Curveæ± å­è¯¦ç»†ä¿¡æ¯")
    print("=" * 80)
    
    # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æ˜¾ç¤º
    for priority in range(1, 6):
        pools_at_priority = {name: info for name, info in AVAILABLE_POOLS.items() 
                           if info['priority'] == priority}
        
        if pools_at_priority:
            priority_labels = {1: "ğŸ† æœ€é«˜ä¼˜å…ˆçº§", 2: "â­ é«˜ä¼˜å…ˆçº§", 3: "ğŸ“ˆ ä¸­ä¼˜å…ˆçº§", 
                             4: "ğŸ“Š ä½ä¼˜å…ˆçº§", 5: "ğŸ”½ æœ€ä½ä¼˜å…ˆçº§"}
            
            print(f"\n{priority_labels.get(priority, f'ä¼˜å…ˆçº§ {priority}')} ({len(pools_at_priority)} ä¸ªæ± å­):")
            print("-" * 60)
            
            for pool_name, pool_info in sorted(pools_at_priority.items()):
                print(f"â€¢ {pool_name:12} | {pool_info['name']:25} | {pool_info['type']:12} | {pool_info['address'][:10]}...")
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»æ± å­æ•°é‡: {len(AVAILABLE_POOLS)}")
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    type_counts = {}
    for pool_info in AVAILABLE_POOLS.values():
        pool_type = pool_info['type']
        type_counts[pool_type] = type_counts.get(pool_type, 0) + 1
    
    print(f"   ç±»å‹åˆ†å¸ƒ:")
    for pool_type, count in sorted(type_counts.items()):
        print(f"     {pool_type}: {count} ä¸ª")
    
    print("\nğŸ¯ æ¨èä½¿ç”¨ç­–ç•¥:")
    print("   â€¢ å¿«é€Ÿæµ‹è¯•: get_high_priority_pools_data() - è·å–ä¼˜å…ˆçº§1-2çš„æ± å­")  
    print("   â€¢ æ—¥å¸¸åˆ†æ: get_all_main_pools_data() - è·å–ä¼˜å…ˆçº§1-3çš„æ± å­")
    print("   â€¢ å…¨é¢åˆ†æ: get_all_pools_data() - è·å–æ‰€æœ‰æ± å­æ•°æ®")
    print("   â€¢ åˆ†ç±»åˆ†æ: get_pools_by_type_data('stable') - æŒ‰ç±»å‹è·å–")
    print("=" * 80)

def switch_days_config(days_setting: str):
    """åˆ‡æ¢å¤©æ•°é…ç½®çš„è¾…åŠ©å‡½æ•°"""
    global CURRENT_DAYS_SETTING
    
    config_map = {
        'quick': QUICK_TEST_DAYS,
        'medium': MEDIUM_RANGE_DAYS, 
        'full': FULL_YEAR_DAYS,
        'test': QUICK_TEST_DAYS,
        '7': QUICK_TEST_DAYS,
        '90': MEDIUM_RANGE_DAYS,
        '365': FULL_YEAR_DAYS
    }
    
    if days_setting.lower() in config_map:
        CURRENT_DAYS_SETTING = config_map[days_setting.lower()]
        print(f"âœ… å·²åˆ‡æ¢åˆ° {CURRENT_DAYS_SETTING} å¤©é…ç½®")
    else:
        print(f"âŒ æ— æ•ˆé…ç½®: {days_setting}")
        print("ğŸ’¡ å¯ç”¨é€‰é¡¹: quick(7å¤©) | medium(90å¤©) | full(365å¤©)")

def demo_all_configurations():
    """æ¼”ç¤ºæ‰€æœ‰é…ç½®é€‰é¡¹"""
    print("ğŸ›ï¸  å¤©æ•°é…ç½®åˆ‡æ¢æ¼”ç¤º")
    print("=" * 40)
    
    configs = [
        ('quick', 'å¿«é€Ÿæµ‹è¯•'),
        ('medium', 'ä¸­æœŸåˆ†æ'), 
        ('full', 'å®Œæ•´å¹´åº¦')
    ]
    
    for config, desc in configs:
        print(f"\nğŸ”„ åˆ‡æ¢åˆ° {desc} æ¨¡å¼:")
        switch_days_config(config)
        
        manager = FreeHistoricalDataManager()
        print(f"ğŸ“Š å°†è·å– {CURRENT_DAYS_SETTING} å¤©æ•°æ®")

if __name__ == "__main__":
    import sys
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ¼”ç¤ºæ¨¡å¼
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        
        if mode == "info":
            show_available_pools_info()
        elif mode == "batch":
            demo_batch_collection_scenarios()
        elif mode == "single":
            demo_free_historical_data()
        elif mode == "all":
            show_available_pools_info()
            demo_free_historical_data()  
            demo_batch_collection_scenarios()
        elif mode == "full":
            # ğŸš€ ç›´æ¥è·å–æ‰€æœ‰æ± å­çš„ä¸€å¹´å†å²æ•°æ®
            print("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰37ä¸ªCurveæ± å­çš„ä¸€å¹´å†å²æ•°æ®...")
            print("âš ï¸  æ³¨æ„: è¿™å°†éœ€è¦è¾ƒé•¿æ—¶é—´ (é¢„è®¡10-20åˆ†é’Ÿ)")
            print("=" * 60)
            
            manager = FreeHistoricalDataManager()
            
            # æ˜¾ç¤ºå°†è¦è·å–çš„æ± å­ä¿¡æ¯
            all_pools = get_pools_by_priority(min_priority=1, max_priority=4)  # è·³è¿‡ä¼˜å…ˆçº§5
            print(f"ğŸ“‹ å°†è·å– {len(all_pools)} ä¸ªæ± å­çš„ {CURRENT_DAYS_SETTING} å¤©æ•°æ®")
            print(f"ğŸ·ï¸  æ± å­ç±»å‹åˆ†å¸ƒ:")
            
            type_counts = {}
            for pool_info in all_pools.values():
                pool_type = pool_info['type']
                type_counts[pool_type] = type_counts.get(pool_type, 0) + 1
            
            for pool_type, count in sorted(type_counts.items()):
                print(f"   {pool_type}: {count} ä¸ª")
                
            # è¯¢é—®ç”¨æˆ·ç¡®è®¤
            response = input("\nç»§ç»­è·å–æ‰€æœ‰æ± å­æ•°æ®? (y/N): ")
            if response.lower() in ['y', 'yes', 'æ˜¯']:
                
                print("\nğŸ”„ å¼€å§‹æ‰¹é‡æ•°æ®è·å–...")
                batch_data = manager.get_all_pools_data(days=CURRENT_DAYS_SETTING, skip_low_priority=True)
                
                # ç»Ÿè®¡ç»“æœ
                successful = sum(1 for df in batch_data.values() if not df.empty)
                total_records = sum(len(df) for df in batch_data.values() if not df.empty)
                
                print(f"\nğŸ‰ æ‰¹é‡è·å–å®Œæˆ!")
                print(f"   âœ… æˆåŠŸ: {successful}/{len(batch_data)} ä¸ªæ± å­")
                print(f"   ğŸ“Š æ€»è®°å½•æ•°: {total_records}")
                
                if successful > 0:
                    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
                    print(f"\nğŸ“ˆ ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š...")
                    analysis = manager.analyze_batch_data(batch_data)
                    
                    # å¯¼å‡ºExcel
                    print(f"\nğŸ“„ å¯¼å‡ºæ•°æ®åˆ°Excel...")
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    excel_path = manager.export_batch_data_to_excel(
                        batch_data,
                        f"curve_all_pools_1year_{timestamp}.xlsx"
                    )
                    
                    if excel_path:
                        print(f"âœ… å®Œæ•´æ•°æ®å·²ä¿å­˜: {excel_path}")
                        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {manager.cache_dir.absolute()}")
                        
                        print(f"\nğŸ’¡ ä½¿ç”¨æ•°æ®:")
                        print(f"from free_historical_data import FreeHistoricalDataManager")
                        print(f"manager = FreeHistoricalDataManager()")
                        print(f"# æ•°æ®å·²ç¼“å­˜ï¼Œä¸‹æ¬¡åŠ è½½ä¼šæ›´å¿«")
                        
                else:
                    print("âŒ æ²¡æœ‰æˆåŠŸè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    
            else:
                print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                
        elif mode == "quick-all":
            # ğŸ”¥ å¿«é€Ÿè·å–æ‰€æœ‰æ± å­çš„7å¤©æ•°æ® (ç”¨äºæµ‹è¯•)
            print("âš¡ å¿«é€Ÿè·å–æ‰€æœ‰æ± å­çš„7å¤©æ•°æ® (æµ‹è¯•æ¨¡å¼)...")
            print("=" * 60)
            
            manager = FreeHistoricalDataManager()
            
            # è·å–7å¤©æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
            batch_data = manager.get_all_pools_data(days=7, skip_low_priority=True)
            
            successful = sum(1 for df in batch_data.values() if not df.empty)
            print(f"\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ: {successful}/{len(batch_data)} ä¸ªæ± å­æˆåŠŸ")
            
            if successful > 0:
                excel_path = manager.export_batch_data_to_excel(
                    batch_data,
                    "curve_all_pools_7d_test.xlsx"
                )
                print(f"ğŸ“„ æµ‹è¯•æ•°æ®å·²å¯¼å‡º: {excel_path}")
                
        else:
            print("Usage: python free_historical_data.py [é€‰é¡¹]")
            print("å¯ç”¨é€‰é¡¹:")
            print("  info      - æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ± å­ä¿¡æ¯")
            print("  batch     - æ¼”ç¤ºæ‰¹é‡æ•°æ®è·å–")  
            print("  single    - æ¼”ç¤ºå•ä¸ªæ± å­è·å–")
            print("  all       - è¿è¡Œæ‰€æœ‰æ¼”ç¤º")
            print("  full      - ğŸš€ è·å–æ‰€æœ‰æ± å­çš„ä¸€å¹´å†å²æ•°æ®")
            print("  quick-all - âš¡ å¿«é€Ÿè·å–æ‰€æœ‰æ± å­çš„7å¤©æ•°æ® (æµ‹è¯•)")
    else:
        # é»˜è®¤è¿è¡Œå•ä¸ªæ± å­æ¼”ç¤º
        demo_free_historical_data() 