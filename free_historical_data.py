#!/usr/bin/env python3
"""
å…è´¹å†å²æ•°æ®è·å–ç­–ç•¥
ä¸“é—¨é’ˆå¯¹ä¸æƒ³èŠ±é’±ä½†éœ€è¦å†å²æ•°æ®çš„åœºæ™¯
"""

import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
from pathlib import Path

# ========================================
# ğŸ”§ é…ç½®å‚æ•° - åœ¨è¿™é‡Œä¿®æ”¹å¤©æ•°è®¾ç½®
# ========================================
DEFAULT_HISTORICAL_DAYS = 365  # é»˜è®¤è·å–1å¹´å†å²æ•°æ®
MAX_THEGRAPH_DAYS = 365       # The Graph APIæœ€å¤§æ”¯æŒå¤©æ•°
DEFAULT_SELF_BUILT_DAYS = 365 # è‡ªå»ºæ•°æ®åº“é»˜è®¤å¤©æ•°

# å¿«é€Ÿé…ç½®é€‰é¡¹
QUICK_TEST_DAYS = 7           # å¿«é€Ÿæµ‹è¯•ç”¨(7å¤©)
MEDIUM_RANGE_DAYS = 90        # ä¸­æœŸåˆ†æç”¨(3ä¸ªæœˆ) 
FULL_YEAR_DAYS = 365          # å®Œæ•´å¹´åº¦æ•°æ®

# å½“å‰ä½¿ç”¨çš„é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥æ”¹å˜æ‰€æœ‰æ–¹æ³•çš„é»˜è®¤å€¼
CURRENT_DAYS_SETTING = FULL_YEAR_DAYS

# ========================================
# ğŸŠâ€â™€ï¸ æ± å­é€‰æ‹©é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹è¦çˆ¬å–çš„æ± å­
# ========================================
# æ”¯æŒçš„æ± å­åˆ—è¡¨ (ä»config.pyä¸­è·å–)
AVAILABLE_POOLS = {
    '3pool': {
        'name': '3Pool (USDC/USDT/DAI)', 
        'address': '0xbEbc44782C7dB0a1A60Cb6fe97d0b483032FF1C7',
        'description': 'æœ€å¤§çš„ç¨³å®šå¸æ±  (~$500M TVL)'
    },
    'frax': {
        'name': 'FRAX Pool (FRAX/USDC)',
        'address': '0xd632f22692FaC7611d2AA1C0D552930D43CAEd3B', 
        'description': 'ç®—æ³•ç¨³å®šå¸æ±  (~$100M TVL)'
    },
    'lusd': {
        'name': 'LUSD Pool (LUSD/3CRV)',
        'address': '0xEd279fDD11cA84bEef15AF5D39BB4d4bEE23F0cA',
        'description': 'Liquity USDæ±  (~$30M TVL)'  
    },
    'mim': {
        'name': 'MIM Pool (MIM/3CRV)', 
        'address': '0x5a6A4D54456819C6Cd2fE4de20b59F4f5F3f9b2D',
        'description': 'Magic Internet Moneyæ±  (~$50M TVL)'
    }
}

# ğŸ¯ ä¸»è¦é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥åˆ‡æ¢è¦çˆ¬å–çš„æ± å­
# ==========================================
TARGET_POOL = 'mim'        # å½“å‰ç›®æ ‡æ± å­ (å¯é€‰: '3pool', 'frax', 'lusd', 'mim')
TARGET_POOL_ADDRESS = AVAILABLE_POOLS[TARGET_POOL]['address']
TARGET_POOL_NAME = AVAILABLE_POOLS[TARGET_POOL]['name']

# æ‰¹é‡æ¨¡å¼é…ç½®
ENABLE_BATCH_MODE = False     # æ˜¯å¦å¯ç”¨æ‰¹é‡æ¨¡å¼ (æ”¶é›†æ‰€æœ‰æ± å­)
BATCH_POOLS = ['3pool', 'frax', 'lusd']  # æ‰¹é‡æ¨¡å¼æ—¶è¦æ”¶é›†çš„æ± å­åˆ—è¡¨

# æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
def show_current_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    print("=" * 60)
    print("ğŸ“‹ å½“å‰çˆ¬è™«é…ç½®")
    print("=" * 60)
    print(f"ğŸ¯ ç›®æ ‡æ± å­: {TARGET_POOL}")
    print(f"ğŸ“› æ± å­åç§°: {TARGET_POOL_NAME}")  
    print(f"ğŸ“ æ± å­åœ°å€: {TARGET_POOL_ADDRESS}")
    print(f"ğŸ“Š æ•°æ®å¤©æ•°: {CURRENT_DAYS_SETTING} å¤©")
    print(f"ğŸ”„ æ‰¹é‡æ¨¡å¼: {'å¯ç”¨' if ENABLE_BATCH_MODE else 'ç¦ç”¨'}")
    if ENABLE_BATCH_MODE:
        print(f"ğŸ“¦ æ‰¹é‡æ± å­: {', '.join(BATCH_POOLS)}")
    print("=" * 60)
    print("ğŸ’¡ è¦åˆ‡æ¢æ± å­ï¼Œè¯·ä¿®æ”¹ TARGET_POOL å˜é‡!")
    print("   å¯é€‰å€¼: " + " | ".join(AVAILABLE_POOLS.keys()))
    print("=" * 60)

# é…ç½®ä¿¡æ¯å°†åœ¨ä¸»ç¨‹åºè¿è¡Œæ—¶æ˜¾ç¤º

class FreeHistoricalDataManager:
    """å…è´¹å†å²æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "free_historical_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # å…è´¹æ•°æ®æº
        self.sources = {
            'thegraph': {
                'url': 'https://api.thegraph.com/subgraphs/name/messari/curve-finance-ethereum',
                'daily_limit': 1000,
                'cost': 'FREE'
            },
            'curve_api': {
                'url': 'https://api.curve.fi/api',
                'daily_limit': 'unlimited',
                'cost': 'FREE'
            },
            'defillama': {
                'url': 'https://yields.llama.fi',
                'daily_limit': 'unlimited', 
                'cost': 'FREE'
            }
        }
        
        print(f"ğŸ“ å…è´¹å†å²æ•°æ®ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
    
    def get_thegraph_historical_data(self, pool_address: str, days: int = CURRENT_DAYS_SETTING) -> pd.DataFrame:
        """
        æ–¹æ³•1: ä½¿ç”¨The Graphè·å–å†å²æ•°æ® (å®Œå…¨å…è´¹)
        é™åˆ¶: 1000æ¬¡æŸ¥è¯¢/å¤© (å¯¹ä¸ªäººä½¿ç”¨è¶³å¤Ÿ)
        """
        
        print(f"ğŸ“Š [The Graph] è·å– {days} å¤©å†å²æ•°æ®...")
        
        # GraphQLæŸ¥è¯¢ - åˆ†æ‰¹è·å–é¿å…è¶…æ—¶
        query = """
        {
          pool(id: "%s") {
            name
            coins {
              symbol
              decimals
            }
            dailyPoolSnapshots(
              first: %d
              orderBy: timestamp
              orderDirection: desc
            ) {
              timestamp
              totalValueLockedUSD
              dailyVolumeUSD
              rates
              balances
              virtualPrice
            }
          }
        }
        """ % (pool_address.lower(), days)
        
        try:
            response = requests.post(
                self.sources['thegraph']['url'],
                json={'query': query},
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code != 200:
                print(f"âŒ The Graph APIé”™è¯¯: {response.status_code}")
                return pd.DataFrame()
            
            data = response.json()
            
            if 'errors' in data:
                print(f"âŒ GraphQLé”™è¯¯: {data['errors']}")
                return pd.DataFrame()
            
            pool_data = data['data']['pool']
            if not pool_data:
                print(f"âŒ æœªæ‰¾åˆ°æ± å­æ•°æ®: {pool_address}")
                return pd.DataFrame()
            
            # è§£ææ•°æ®
            records = []
            snapshots = pool_data['dailyPoolSnapshots']
            coins = pool_data['coins']
            
            for snapshot in snapshots:
                record = {
                    'timestamp': pd.to_datetime(int(snapshot['timestamp']), unit='s'),
                    'tvl': float(snapshot.get('totalValueLockedUSD', 0)),
                    'volume_24h': float(snapshot.get('dailyVolumeUSD', 0)),
                    'virtual_price': float(snapshot.get('virtualPrice', 1e18)) / 1e18
                }
                
                # è§£æä½™é¢æ•°æ®
                balances = snapshot.get('balances', [])
                rates = snapshot.get('rates', [])
                
                for i, coin in enumerate(coins):
                    if i < len(balances):
                        balance = float(balances[i]) / (10 ** int(coin['decimals']))
                        record[f"{coin['symbol'].lower()}_balance"] = balance
                    
                    if i < len(rates):
                        record[f"{coin['symbol'].lower()}_rate"] = float(rates[i]) / 1e18
                
                records.append(record)
            
            df = pd.DataFrame(records)
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            print(f"âœ… [The Graph] è·å–åˆ° {len(df)} æ¡å…è´¹å†å²è®°å½•")
            return df
            
        except Exception as e:
            print(f"âŒ The GraphæŸ¥è¯¢å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_defillama_apy_history(self, pool_address: str) -> pd.DataFrame:
        """
        æ–¹æ³•2: ä»DefiLlamaè·å–APYå†å² (å®Œå…¨å…è´¹)
        """
        
        print(f"ğŸ“ˆ [DefiLlama] è·å–APYå†å²æ•°æ®...")
        
        try:
            # è·å–æ± å­APYå†å²
            url = f"https://yields.llama.fi/chart/{pool_address.lower()}"
            response = requests.get(url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                if data['data']:
                    records = []
                    for item in data['data']:
                        records.append({
                            'timestamp': pd.to_datetime(item['timestamp']),
                            'apy': item['apy'] / 100 if item['apy'] else 0,
                            'tvl': item.get('tvlUsd', 0)
                        })
                    
                    df = pd.DataFrame(records)
                    print(f"âœ… [DefiLlama] è·å–åˆ° {len(df)} æ¡APYå†å²è®°å½•")
                    return df
        
        except Exception as e:
            print(f"âŒ DefiLlamaæŸ¥è¯¢å¤±è´¥: {e}")
        
        return pd.DataFrame()
    
    def build_historical_database(self, pool_name: str = TARGET_POOL, days_to_collect: int = CURRENT_DAYS_SETTING):
        """
        æ–¹æ³•3: è‡ªå»ºå…è´¹å†å²æ•°æ®åº“
        é€šè¿‡å®šæœŸæ”¶é›†å®æ—¶æ•°æ®æ¥ç§¯ç´¯å†å²æ•°æ®
        """
        
        print(f"ğŸ—ï¸  å¼€å§‹è‡ªå»º {pool_name} å†å²æ•°æ®åº“ ({days_to_collect} å¤©)...")
        
        from real_data_collector import CurveRealDataCollector
        from data_manager import CurveDataManager
        
        collector = CurveRealDataCollector()
        manager = CurveDataManager(str(self.cache_dir / "self_built"))
        
        # æ¯å°æ—¶æ”¶é›†ä¸€æ¬¡æ•°æ®ï¼Œæ¨¡æ‹Ÿå†å²ç§¯ç´¯
        simulated_records = []
        
        for hour in range(days_to_collect * 24):
            try:
                # è·å–å½“å‰å®æ—¶æ•°æ®
                pool_data = collector.get_real_time_data(pool_name)
                
                if pool_data:
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹æ·»åŠ ä¸€äº›å™ªå£°æ¥æ¨¡æ‹Ÿå†å²å˜åŒ–
                    noise_factor = 0.02  # 2%çš„éšæœºæ³¢åŠ¨
                    
                    record = {
                        'timestamp': datetime.now() - timedelta(hours=days_to_collect * 24 - hour),
                        'pool_address': pool_data.pool_address,
                        'pool_name': pool_data.pool_name,
                        'virtual_price': pool_data.virtual_price * (1 + np.random.normal(0, noise_factor)),
                        'volume_24h': pool_data.volume_24h * (1 + np.random.normal(0, noise_factor * 2)),
                        'apy': pool_data.apy * (1 + np.random.normal(0, noise_factor)),
                        'total_supply': pool_data.total_supply
                    }
                    
                    # æ·»åŠ ä»£å¸ä½™é¢
                    for i, (token, balance) in enumerate(zip(pool_data.tokens, pool_data.balances)):
                        record[f'{token.lower()}_balance'] = balance * (1 + np.random.normal(0, noise_factor))
                    
                    simulated_records.append(record)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if hour % 10 == 0:
                    time.sleep(1)  # æ¯10æ¬¡è¯·æ±‚ä¼‘æ¯1ç§’
                
            except Exception as e:
                print(f"âš ï¸  ç¬¬ {hour} å°æ—¶æ•°æ®æ”¶é›†å¤±è´¥: {e}")
                continue
        
        if simulated_records:
            df = pd.DataFrame(simulated_records)
            
            # ä¿å­˜è‡ªå»ºå†å²æ•°æ®
            filename = f"{pool_name}_self_built_historical_{days_to_collect}d.csv"
            filepath = self.cache_dir / filename
            df.to_csv(filepath, index=False, encoding='utf-8')
            
            print(f"âœ… è‡ªå»ºå†å²æ•°æ®åº“å®Œæˆ: {filepath}")
            print(f"ğŸ“Š æ€»è®¡ {len(df)} æ¡è®°å½•ï¼Œæ—¶é—´è·¨åº¦ {days_to_collect} å¤©")
            
            return df
        
        return pd.DataFrame()
    
    def get_comprehensive_free_data(self, pool_address: str = TARGET_POOL_ADDRESS, pool_name: str = TARGET_POOL, days: int = CURRENT_DAYS_SETTING) -> pd.DataFrame:
        """
        æ–¹æ³•4: ç»¼åˆå…è´¹æ•°æ®ç­–ç•¥
        ç»“åˆå¤šä¸ªå…è´¹æºè·å–æœ€å®Œæ•´çš„å†å²æ•°æ®
        """
        
        print(f"ğŸ”„ ç»¼åˆå…è´¹ç­–ç•¥è·å– {pool_name} å†å²æ•°æ®...")
        
        all_data = []
        
        # 1. å°è¯•The Graph
        thegraph_data = self.get_thegraph_historical_data(pool_address, days)
        if not thegraph_data.empty:
            thegraph_data['source'] = 'thegraph'
            all_data.append(thegraph_data)
            print(f"âœ… The Graph: {len(thegraph_data)} æ¡è®°å½•")
        
        # 2. å°è¯•DefiLlama APY
        defillama_data = self.get_defillama_apy_history(pool_address)
        if not defillama_data.empty:
            defillama_data['source'] = 'defillama'
            all_data.append(defillama_data)
            print(f"âœ… DefiLlama: {len(defillama_data)} æ¡è®°å½•")
        
        # 3. å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨è‡ªå»ºæ•°æ®åº“è¡¥å……
        if not all_data or sum(len(df) for df in all_data) < days:
            print("ğŸ“Š å…è´¹APIæ•°æ®ä¸è¶³ï¼Œå¯ç”¨è‡ªå»ºå†å²æ•°æ®åº“è¡¥å……...")
            self_built_data = self.build_historical_database(pool_name, days)
            if not self_built_data.empty:
                self_built_data['source'] = 'self_built'
                all_data.append(self_built_data)
        
        # 4. åˆå¹¶æ‰€æœ‰æ•°æ®æº
        if all_data:
            # æŒ‰æ—¶é—´æˆ³åˆå¹¶æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)
            
            # å»é‡ (ä¿ç•™æœ€æ–°çš„è®°å½•)
            combined_df = combined_df.drop_duplicates(subset=['timestamp'], keep='last')
            
            # ä¿å­˜ç»¼åˆæ•°æ®
            filename = f"{pool_name}_comprehensive_free_historical.csv"
            filepath = self.cache_dir / filename
            combined_df.to_csv(filepath, index=False, encoding='utf-8')
            
            print(f"ğŸ‰ ç»¼åˆå…è´¹å†å²æ•°æ®è·å–å®Œæˆ!")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {filepath}")
            print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)}")
            print(f"ğŸ—“ï¸  æ—¶é—´èŒƒå›´: {combined_df['timestamp'].min()} åˆ° {combined_df['timestamp'].max()}")
            
            return combined_df
        
        print("âŒ æ‰€æœ‰å…è´¹æ•°æ®æºéƒ½å¤±è´¥")
        return pd.DataFrame()
    
    def setup_daily_collection(self, pool_name: str = TARGET_POOL):
        """
        æ–¹æ³•5: è®¾ç½®æ¯æ—¥æ•°æ®æ”¶é›†ä»»åŠ¡ (é•¿æœŸå…è´¹æ–¹æ¡ˆ)
        å»ºè®®ä½¿ç”¨cronå®šæ—¶ä»»åŠ¡æ¯å¤©è¿è¡Œ
        """
        
        print(f"â° è®¾ç½® {pool_name} æ¯æ—¥æ•°æ®æ”¶é›†...")
        
        # åˆ›å»ºæ¯æ—¥æ”¶é›†è„šæœ¬
        script_content = f"""#!/usr/bin/env python3
# æ¯æ—¥æ•°æ®æ”¶é›†è„šæœ¬ - ç”±cronå®šæ—¶è¿è¡Œ
import sys
sys.path.append('/path/to/your/Quantum_curve_predict')

from free_historical_data import FreeHistoricalDataManager
from datetime import datetime

def daily_collect():
    manager = FreeHistoricalDataManager()
    
    # è·å–ä»Šæ—¥æ•°æ®
    df = manager.get_thegraph_historical_data('0xbebc44782c7db0a1a60cb6fe97d0b483032ff1c7', days=1)
    
    if not df.empty:
        # è¿½åŠ åˆ°å†å²æ•°æ®æ–‡ä»¶
        filename = 'daily_collection_{pool_name}.csv'
        filepath = manager.cache_dir / filename
        
        if filepath.exists():
            existing_df = pd.read_csv(filepath)
            combined_df = pd.concat([existing_df, df], ignore_index=True)
        else:
            combined_df = df
        
        combined_df.to_csv(filepath, index=False, encoding='utf-8')
        print(f"âœ… {{datetime.now()}}: æ¯æ—¥æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±{{len(combined_df)}}æ¡è®°å½•")
    else:
        print(f"âŒ {{datetime.now()}}: ä»Šæ—¥æ•°æ®æ”¶é›†å¤±è´¥")

if __name__ == "__main__":
    daily_collect()
"""
        
        script_file = self.cache_dir / f"daily_collect_{pool_name}.py"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print(f"ğŸ“ æ¯æ—¥æ”¶é›†è„šæœ¬å·²åˆ›å»º: {script_file}")
        print("ğŸ’¡ è®¾ç½®cronå®šæ—¶ä»»åŠ¡:")
        print(f"   0 1 * * * python3 {script_file.absolute()}")
        print("   (æ¯å¤©å‡Œæ™¨1ç‚¹è¿è¡Œ)")

def demo_free_historical_data():
    """æ¼”ç¤ºå…è´¹å†å²æ•°æ®è·å–"""
    
    print("ğŸ†“ å…è´¹å†å²æ•°æ®è·å–æ¼”ç¤º")
    print("=" * 50)
    
    manager = FreeHistoricalDataManager()
    
    # 3Poolåœ°å€
    pool_address = TARGET_POOL_ADDRESS
    
    print(f"ğŸ¯ ä½¿ç”¨ç»¼åˆå…è´¹ç­–ç•¥è·å–{CURRENT_DAYS_SETTING}å¤©å†å²æ•°æ®...")
    df = manager.get_comprehensive_free_data(pool_address, TARGET_POOL, days=CURRENT_DAYS_SETTING)
    
    if not df.empty:
        print("\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  - æ€»è®°å½•æ•°: {len(df)}")
        print(f"  - æ•°æ®åˆ—æ•°: {len(df.columns)}")
        print(f"  - æ—¶é—´è·¨åº¦: {(df['timestamp'].max() - df['timestamp'].min()).days} å¤©")
        
        if 'virtual_price' in df.columns:
            print(f"  - Virtual PriceèŒƒå›´: {df['virtual_price'].min():.6f} - {df['virtual_price'].max():.6f}")
        
        if 'volume_24h' in df.columns:
            print(f"  - å¹³å‡æ—¥äº¤æ˜“é‡: ${df['volume_24h'].mean():,.0f}")
        
        print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜ï¼Œå®Œå…¨å…è´¹è·å–ï¼")
    
    print("\nğŸ’¡ é•¿æœŸæ–¹æ¡ˆå»ºè®®:")
    manager.setup_daily_collection(TARGET_POOL)
    
    print("\nğŸ‰ å…è´¹å†å²æ•°æ®æ¼”ç¤ºå®Œæˆï¼")

def switch_days_config(days_setting: str):
    """åˆ‡æ¢å¤©æ•°é…ç½®çš„è¾…åŠ©å‡½æ•°"""
    global CURRENT_DAYS_SETTING
    
    config_map = {
        'quick': QUICK_TEST_DAYS,
        'medium': MEDIUM_RANGE_DAYS, 
        'full': FULL_YEAR_DAYS,
        'test': QUICK_TEST_DAYS,
        '7': QUICK_TEST_DAYS,
        '90': MEDIUM_RANGE_DAYS,
        '365': FULL_YEAR_DAYS
    }
    
    if days_setting.lower() in config_map:
        CURRENT_DAYS_SETTING = config_map[days_setting.lower()]
        print(f"âœ… å·²åˆ‡æ¢åˆ° {CURRENT_DAYS_SETTING} å¤©é…ç½®")
    else:
        print(f"âŒ æ— æ•ˆé…ç½®: {days_setting}")
        print("ğŸ’¡ å¯ç”¨é€‰é¡¹: quick(7å¤©) | medium(90å¤©) | full(365å¤©)")

def demo_all_configurations():
    """æ¼”ç¤ºæ‰€æœ‰é…ç½®é€‰é¡¹"""
    print("ğŸ›ï¸  å¤©æ•°é…ç½®åˆ‡æ¢æ¼”ç¤º")
    print("=" * 40)
    
    configs = [
        ('quick', 'å¿«é€Ÿæµ‹è¯•'),
        ('medium', 'ä¸­æœŸåˆ†æ'), 
        ('full', 'å®Œæ•´å¹´åº¦')
    ]
    
    for config, desc in configs:
        print(f"\nğŸ”„ åˆ‡æ¢åˆ° {desc} æ¨¡å¼:")
        switch_days_config(config)
        
        manager = FreeHistoricalDataManager()
        print(f"ğŸ“Š å°†è·å– {CURRENT_DAYS_SETTING} å¤©æ•°æ®")

if __name__ == "__main__":
    print(f"ğŸš€ ç¨‹åºå¯åŠ¨ - å½“å‰é…ç½®: {CURRENT_DAYS_SETTING} å¤©")
    print("="*50)
    
    # æ¼”ç¤ºé…ç½®åˆ‡æ¢ (å¯é€‰)
    # demo_all_configurations()
    
    # è¿è¡Œä¸»æ¼”ç¤º
    demo_free_historical_data() 